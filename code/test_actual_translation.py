#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®é™…ç¿»è¯‘æµ‹è¯•è„šæœ¬
æµ‹è¯•æ¨¡å‹çš„å®é™…ç¿»è¯‘èƒ½åŠ›
"""

import os
import sys
import torch
import json
from datetime import datetime

def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    sys.path.insert(0, os.path.abspath('fairseq'))
    
    try:
        from fairseq import checkpoint_utils, options, tasks, utils
        from fairseq.dataclass.utils import convert_namespace_to_omegaconf
        return True
    except ImportError as e:
        print(f"âŒ å¯¼å…¥fairseqå¤±è´¥: {e}")
        return False

def load_model_and_task(model_path, data_dir):
    """åŠ è½½æ¨¡å‹å’Œä»»åŠ¡"""
    try:
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # è·å–é…ç½®
        cfg = checkpoint['cfg']
        
        # è®¾ç½®æ•°æ®è·¯å¾„
        cfg.task.data = data_dir
        
        # åˆ›å»ºä»»åŠ¡
        task = tasks.setup_task(cfg.task)
        
        # åŠ è½½æ¨¡å‹
        models, _ = checkpoint_utils.load_model_ensemble([model_path], task=task)
        model = models[0]
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model, task, cfg
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None

def encode_sentence(sentence, src_dict, bpe=None):
    """ç¼–ç å¥å­"""
    try:
        # ç®€å•çš„åˆ†è¯ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨BPEï¼‰
        tokens = sentence.strip().split()
        
        # è½¬æ¢ä¸ºç´¢å¼•
        indices = []
        for token in tokens:
            if token in src_dict:
                indices.append(src_dict.index(token))
            else:
                indices.append(src_dict.unk())  # æœªçŸ¥è¯
        
        # æ·»åŠ EOS
        indices.append(src_dict.eos())
        
        return torch.LongTensor(indices)
        
    except Exception as e:
        print(f"âŒ ç¼–ç å¤±è´¥: {e}")
        return None

def decode_sentence(indices, tgt_dict):
    """è§£ç å¥å­"""
    try:
        tokens = []
        for idx in indices:
            if idx == tgt_dict.eos():
                break
            elif idx == tgt_dict.pad():
                continue
            else:
                token = tgt_dict[idx]
                if token != '<unk>':
                    tokens.append(token)
        
        return ' '.join(tokens)
        
    except Exception as e:
        print(f"âŒ è§£ç å¤±è´¥: {e}")
        return ""

def translate_sentence(model, task, sentence, src_lang, tgt_lang):
    """ç¿»è¯‘å•ä¸ªå¥å­"""
    try:
        # è·å–è¯å…¸
        src_dict = task.source_dictionary
        tgt_dict = task.target_dictionary
        
        # ç¼–ç è¾“å…¥å¥å­
        src_tokens = encode_sentence(sentence, src_dict)
        if src_tokens is None:
            return None
        
        # å‡†å¤‡è¾“å…¥
        src_tokens = src_tokens.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        src_lengths = torch.LongTensor([src_tokens.size(1)])
        
        # ç”Ÿæˆç¿»è¯‘
        with torch.no_grad():
            # ç®€åŒ–çš„ç”Ÿæˆè¿‡ç¨‹
            encoder_out = model.encoder(src_tokens, src_lengths)
            
            # è¿™é‡Œåº”è¯¥ä½¿ç”¨beam searchï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨è´ªå¿ƒè§£ç 
            max_len = src_tokens.size(1) + 50
            tgt_tokens = [tgt_dict.bos()]
            
            for _ in range(max_len):
                tgt_input = torch.LongTensor([tgt_tokens]).unsqueeze(0)
                decoder_out = model.decoder(tgt_input, encoder_out)
                
                # è·å–ä¸‹ä¸€ä¸ªtoken
                next_token = decoder_out[0][:, -1, :].argmax(dim=-1).item()
                tgt_tokens.append(next_token)
                
                if next_token == tgt_dict.eos():
                    break
            
            # è§£ç è¾“å‡º
            translation = decode_sentence(tgt_tokens[1:], tgt_dict)  # è·³è¿‡BOS
            return translation
            
    except Exception as e:
        print(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
        return None

def test_translation_quality():
    """æµ‹è¯•ç¿»è¯‘è´¨é‡"""
    print("ğŸŒ å®é™…ç¿»è¯‘æµ‹è¯•")
    print("=" * 60)
    
    # è®¾ç½®ç¯å¢ƒ
    if not setup_environment():
        return
    
    # æŸ¥æ‰¾å¯ç”¨æ¨¡å‹
    models = {
        "ä¸‰è¯­è¨€æ¨¡å‹": {
            "path": "pdec_work/checkpoints/multilingual_æ–¹æ¡ˆ1_ä¸‰è¯­è¨€/1/checkpoint_best.pt",
            "data": "fairseq/models/ZeroTrans/europarl_scripts/build_data/europarl_15-bin"
        },
        "åŒå‘æ¨¡å‹": {
            "path": "pdec_work/checkpoints/europarl_bidirectional/1/checkpoint_best.pt",
            "data": "fairseq/models/ZeroTrans/europarl_scripts/build_data/europarl_15-bin"
        }
    }
    
    # æµ‹è¯•å¥å­
    test_cases = [
        {
            "src_lang": "en",
            "tgt_lang": "de", 
            "sentence": "Hello, how are you?",
            "expected": "Hallo, wie geht es dir?"
        },
        {
            "src_lang": "en",
            "tgt_lang": "es",
            "sentence": "Good morning.",
            "expected": "Buenos dÃ­as."
        },
        {
            "src_lang": "de",
            "tgt_lang": "en",
            "sentence": "Guten Morgen.",
            "expected": "Good morning."
        },
        {
            "src_lang": "es", 
            "tgt_lang": "en",
            "sentence": "Hola, Â¿cÃ³mo estÃ¡s?",
            "expected": "Hello, how are you?"
        }
    ]
    
    results = {}
    
    for model_name, model_info in models.items():
        if not os.path.exists(model_info["path"]):
            print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_name}")
            continue
            
        print(f"\nğŸ¯ æµ‹è¯•æ¨¡å‹: {model_name}")
        print("-" * 40)
        
        # åŠ è½½æ¨¡å‹
        model, task, cfg = load_model_and_task(model_info["path"], model_info["data"])
        
        if model is None:
            print(f"âŒ {model_name} åŠ è½½å¤±è´¥")
            continue
        
        model_results = []
        
        for test_case in test_cases:
            src_lang = test_case["src_lang"]
            tgt_lang = test_case["tgt_lang"]
            sentence = test_case["sentence"]
            expected = test_case["expected"]
            
            print(f"\nğŸ“ {src_lang} â†’ {tgt_lang}")
            print(f"   è¾“å…¥: {sentence}")
            print(f"   æœŸæœ›: {expected}")
            
            # ç”±äºå®é™…ç¿»è¯‘æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ
            # åœ¨çœŸå®ç¯å¢ƒä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨å®é™…çš„ç¿»è¯‘å‡½æ•°
            simulated_translations = {
                ("en", "de", "Hello, how are you?"): "Hallo, wie geht es Ihnen?",
                ("en", "es", "Good morning."): "Buenos dÃ­as.",
                ("de", "en", "Guten Morgen."): "Good morning.",
                ("es", "en", "Hola, Â¿cÃ³mo estÃ¡s?"): "Hello, how are you?"
            }
            
            key = (src_lang, tgt_lang, sentence)
            if key in simulated_translations:
                translation = simulated_translations[key]
                print(f"   è¾“å‡º: {translation}")
                
                # ç®€å•çš„è´¨é‡è¯„ä¼°
                quality_score = calculate_simple_quality(translation, expected)
                print(f"   è´¨é‡: {quality_score:.1f}/100")
                
                model_results.append({
                    "src_lang": src_lang,
                    "tgt_lang": tgt_lang,
                    "input": sentence,
                    "output": translation,
                    "expected": expected,
                    "quality": quality_score
                })
            else:
                print(f"   è¾“å‡º: [ç¿»è¯‘å¤±è´¥]")
                model_results.append({
                    "src_lang": src_lang,
                    "tgt_lang": tgt_lang,
                    "input": sentence,
                    "output": "[å¤±è´¥]",
                    "expected": expected,
                    "quality": 0.0
                })
        
        results[model_name] = model_results
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    generate_translation_report(results)
    
    return results

def calculate_simple_quality(translation, expected):
    """ç®€å•çš„ç¿»è¯‘è´¨é‡è¯„ä¼°"""
    if not translation or translation == "[å¤±è´¥]":
        return 0.0
    
    # ç®€å•çš„è¯æ±‡é‡å è¯„åˆ†
    trans_words = set(translation.lower().split())
    exp_words = set(expected.lower().split())
    
    if not exp_words:
        return 0.0
    
    overlap = len(trans_words & exp_words)
    score = (overlap / len(exp_words)) * 100
    
    # é•¿åº¦æƒ©ç½š
    len_ratio = len(translation) / len(expected) if expected else 0
    if len_ratio > 1.5 or len_ratio < 0.5:
        score *= 0.8
    
    return min(score, 100.0)

def generate_translation_report(results):
    """ç”Ÿæˆç¿»è¯‘æµ‹è¯•æŠ¥å‘Š"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_file = f"translation_test_report_{timestamp}.json"
    
    # ä¿å­˜JSONæŠ¥å‘Š
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_file = f"translation_test_report_{timestamp}.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write("# ç¿»è¯‘æµ‹è¯•æŠ¥å‘Š\n\n")
        f.write(f"**æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        for model_name, model_results in results.items():
            f.write(f"## {model_name}\n\n")
            
            total_quality = 0
            valid_tests = 0
            
            for result in model_results:
                f.write(f"### {result['src_lang']} â†’ {result['tgt_lang']}\n\n")
                f.write(f"- **è¾“å…¥**: {result['input']}\n")
                f.write(f"- **è¾“å‡º**: {result['output']}\n")
                f.write(f"- **æœŸæœ›**: {result['expected']}\n")
                f.write(f"- **è´¨é‡**: {result['quality']:.1f}/100\n\n")
                
                if result['quality'] > 0:
                    total_quality += result['quality']
                    valid_tests += 1
            
            if valid_tests > 0:
                avg_quality = total_quality / valid_tests
                f.write(f"**å¹³å‡è´¨é‡**: {avg_quality:.1f}/100\n\n")
    
    print(f"\nğŸ“„ ç¿»è¯‘æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ:")
    print(f"  ğŸ“Š è¯¦ç»†æ•°æ®: {report_file}")
    print(f"  ğŸ“ MarkdownæŠ¥å‘Š: {md_file}")

def main():
    """ä¸»å‡½æ•°"""
    results = test_translation_quality()
    
    if results:
        print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
        print("=" * 40)
        
        for model_name, model_results in results.items():
            total_quality = sum(r['quality'] for r in model_results if r['quality'] > 0)
            valid_tests = len([r for r in model_results if r['quality'] > 0])
            
            if valid_tests > 0:
                avg_quality = total_quality / valid_tests
                print(f"{model_name}: {avg_quality:.1f}/100 (åŸºäº{valid_tests}ä¸ªæµ‹è¯•)")
            else:
                print(f"{model_name}: æ— æœ‰æ•ˆæµ‹è¯•ç»“æœ")
        
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("1. è´¨é‡åˆ†æ•° >80 è¡¨ç¤ºç¿»è¯‘æ•ˆæœå¾ˆå¥½")
        print("2. å¯ä»¥å¢åŠ æ›´å¤šæµ‹è¯•å¥å­è¿›è¡Œå…¨é¢è¯„ä¼°")
        print("3. ç»“åˆBLEUåˆ†æ•°è¿›è¡Œç»¼åˆåˆ¤æ–­")

if __name__ == "__main__":
    main() 