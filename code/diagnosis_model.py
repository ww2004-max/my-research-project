#!/usr/bin/env python3
"""
æ¨¡å‹è¯Šæ–­è„šæœ¬ - æµ‹è¯•ä¸åŒçš„è§£ç ç­–ç•¥
"""

import torch
import torch.nn.functional as F
from fairseq import checkpoint_utils, options, tasks, utils
from fairseq.data import encoders
import numpy as np
import os

def load_model_and_task():
    """åŠ è½½æ¨¡å‹å’Œä»»åŠ¡"""
    print("ğŸ” åŠ è½½æ¨¡å‹å’Œä»»åŠ¡...")
    
    # è®¾ç½®å‚æ•°
    checkpoint_path = "pdec_work/checkpoints/europarl_test/1/checkpoint_best_fixed.pt"
    data_path = "fairseq/models/ZeroTrans/europarl_scripts/build_data/europarl_15-bin"
    
    # åŠ è½½checkpoint
    state = checkpoint_utils.load_checkpoint_to_cpu(checkpoint_path)
    cfg = state['cfg']
    
    # è®¾ç½®ä»»åŠ¡
    task = tasks.setup_task(cfg.task)
    
    # åŠ è½½æ•°æ®
    task.load_dataset('train', combine=False, epoch=1)
    
    # æ„å»ºæ¨¡å‹
    models, _model_args = checkpoint_utils.load_model_ensemble([checkpoint_path], task=task)
    model = models[0]
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    if torch.cuda.is_available():
        model.cuda()
    
    return model, task, cfg

def test_different_decoding_strategies(model, task, text="The European Parliament"):
    """æµ‹è¯•ä¸åŒçš„è§£ç ç­–ç•¥"""
    print(f"\nğŸ§ª æµ‹è¯•ä¸åŒè§£ç ç­–ç•¥: '{text}'")
    print("=" * 60)
    
    # ç¼–ç è¾“å…¥
    src_tokens = task.source_dictionary.encode_line(text, add_if_not_exist=False).long().unsqueeze(0)
    if torch.cuda.is_available():
        src_tokens = src_tokens.cuda()
    
    print(f"ğŸ“ è¾“å…¥tokens: {src_tokens[0].tolist()}")
    
    # 1. è´ªå©ªè§£ç  (beam_size=1)
    print("\nã€ç­–ç•¥1: è´ªå©ªè§£ç ã€‘")
    try:
        sample = {'net_input': {'src_tokens': src_tokens, 'src_lengths': torch.LongTensor([src_tokens.size(1)])}}
        if torch.cuda.is_available():
            sample['net_input']['src_lengths'] = sample['net_input']['src_lengths'].cuda()
        
        with torch.no_grad():
            translations = task.inference_step(model, sample, prefix_tokens=None)
        
        for i, hypo in enumerate(translations[0][:1]):  # åªå–æœ€å¥½çš„
            tokens = hypo['tokens'].cpu()
            score = hypo['score']
            translation = task.target_dictionary.string(tokens, bpe_symbol='@@ ')
            print(f"   åˆ†æ•°: {score:.4f}")
            print(f"   ç»“æœ: {translation}")
    except Exception as e:
        print(f"   âŒ é”™è¯¯: {e}")
    
    # 2. æµ‹è¯•ä¸åŒçš„beam size
    for beam_size in [3, 5]:
        print(f"\nã€ç­–ç•¥2: Beam Search (beam_size={beam_size})ã€‘")
        try:
            # ä¿®æ”¹é…ç½®
            task.cfg.generation.beam = beam_size
            sample = {'net_input': {'src_tokens': src_tokens, 'src_lengths': torch.LongTensor([src_tokens.size(1)])}}
            if torch.cuda.is_available():
                sample['net_input']['src_lengths'] = sample['net_input']['src_lengths'].cuda()
            
            with torch.no_grad():
                translations = task.inference_step(model, sample, prefix_tokens=None)
            
            for i, hypo in enumerate(translations[0][:3]):  # å–å‰3ä¸ª
                tokens = hypo['tokens'].cpu()
                score = hypo['score']
                translation = task.target_dictionary.string(tokens, bpe_symbol='@@ ')
                print(f"   å€™é€‰{i+1} åˆ†æ•°: {score:.4f} -> {translation}")
        except Exception as e:
            print(f"   âŒ é”™è¯¯: {e}")
    
    # 3. æµ‹è¯•éšæœºé‡‡æ ·
    print(f"\nã€ç­–ç•¥3: éšæœºé‡‡æ ·ã€‘")
    try:
        # ç›´æ¥ä½¿ç”¨æ¨¡å‹å‰å‘ä¼ æ’­
        with torch.no_grad():
            encoder_out = model.encoder(src_tokens)
            
            # æ‰‹åŠ¨è§£ç ï¼Œä½¿ç”¨é‡‡æ ·
            max_len = 50
            tgt_tokens = torch.LongTensor([[task.target_dictionary.eos()]]).cuda() if torch.cuda.is_available() else torch.LongTensor([[task.target_dictionary.eos()]])
            
            for step in range(max_len):
                decoder_out = model.decoder(tgt_tokens, encoder_out)
                logits = decoder_out[0][:, -1, :]  # æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„logits
                
                # åº”ç”¨æ¸©åº¦é‡‡æ ·
                temperature = 0.8
                probs = F.softmax(logits / temperature, dim=-1)
                
                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                next_token = torch.multinomial(probs, 1)
                tgt_tokens = torch.cat([tgt_tokens, next_token], dim=1)
                
                if next_token.item() == task.target_dictionary.eos():
                    break
            
            translation = task.target_dictionary.string(tgt_tokens[0], bpe_symbol='@@ ')
            print(f"   é‡‡æ ·ç»“æœ: {translation}")
            
    except Exception as e:
        print(f"   âŒ é”™è¯¯: {e}")

def analyze_model_weights(model):
    """åˆ†ææ¨¡å‹æƒé‡åˆ†å¸ƒ"""
    print("\nğŸ”¬ æ¨¡å‹æƒé‡åˆ†æ")
    print("=" * 60)
    
    # æ£€æŸ¥è¾“å‡ºå±‚æƒé‡
    if hasattr(model.decoder, 'output_projection'):
        output_weights = model.decoder.output_projection.weight
        print(f"ğŸ“Š è¾“å‡ºå±‚æƒé‡å½¢çŠ¶: {output_weights.shape}")
        print(f"ğŸ“Š æƒé‡å‡å€¼: {output_weights.mean().item():.6f}")
        print(f"ğŸ“Š æƒé‡æ ‡å‡†å·®: {output_weights.std().item():.6f}")
        print(f"ğŸ“Š æƒé‡æœ€å¤§å€¼: {output_weights.max().item():.6f}")
        print(f"ğŸ“Š æƒé‡æœ€å°å€¼: {output_weights.min().item():.6f}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸çš„æƒé‡åˆ†å¸ƒ
        if output_weights.std().item() < 0.01:
            print("âš ï¸  è­¦å‘Š: è¾“å‡ºå±‚æƒé‡æ ‡å‡†å·®å¾ˆå°ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜")
        
        # æ£€æŸ¥ç‰¹å®štokençš„æƒé‡
        common_tokens = [2, 3, 27]  # </s>, <unk>, .
        for token_id in common_tokens:
            if token_id < output_weights.shape[0]:
                token_weights = output_weights[token_id]
                print(f"ğŸ“Š Token {token_id} æƒé‡ç»Ÿè®¡: å‡å€¼={token_weights.mean().item():.6f}, æ ‡å‡†å·®={token_weights.std().item():.6f}")

def test_vocabulary_coverage():
    """æµ‹è¯•è¯æ±‡è¡¨è¦†ç›–ç‡"""
    print("\nğŸ“š è¯æ±‡è¡¨è¦†ç›–ç‡æµ‹è¯•")
    print("=" * 60)
    
    # åŠ è½½å­—å…¸
    data_path = "fairseq/models/ZeroTrans/europarl_scripts/build_data/europarl_15-bin"
    src_dict_path = os.path.join(data_path, "dict.en.txt")
    tgt_dict_path = os.path.join(data_path, "dict.de.txt")
    
    # æµ‹è¯•å¸¸è§è¯æ±‡
    test_words = [
        "the", "and", "of", "to", "a", "in", "is", "it", "you", "that",
        "European", "Parliament", "Commission", "report", "committee",
        "important", "must", "should", "would", "could", "will", "can"
    ]
    
    try:
        from fairseq.data import Dictionary
        src_dict = Dictionary.load(src_dict_path)
        
        print("ğŸ” è‹±è¯­è¯æ±‡è¦†ç›–ç‡:")
        covered = 0
        for word in test_words:
            token_id = src_dict.index(word)
            if token_id != src_dict.unk():
                print(f"   âœ… '{word}' -> {token_id}")
                covered += 1
            else:
                print(f"   âŒ '{word}' -> <unk>")
        
        print(f"\nğŸ“Š è¦†ç›–ç‡: {covered}/{len(test_words)} ({covered/len(test_words)*100:.1f}%)")
        
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½è¯å…¸: {e}")

def main():
    print("ğŸ”¬ PhasedDecoderæ¨¡å‹è¯Šæ–­")
    print("=" * 60)
    
    try:
        # åŠ è½½æ¨¡å‹
        model, task, cfg = load_model_and_task()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•è¯æ±‡è¡¨
        test_vocabulary_coverage()
        
        # åˆ†ææ¨¡å‹æƒé‡
        analyze_model_weights(model)
        
        # æµ‹è¯•ä¸åŒè§£ç ç­–ç•¥
        test_sentences = [
            "The European Parliament",
            "We must consider",
            "This is important",
            "The report shows"
        ]
        
        for sentence in test_sentences:
            test_different_decoding_strategies(model, task, sentence)
        
        print("\nğŸ‰ è¯Šæ–­å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ è¯Šæ–­è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 