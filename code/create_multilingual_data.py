#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ›å»ºå¤šè¯­è¨€ç¿»è¯‘æ•°æ®
"""

import os
import shutil

def create_multilingual_pairs():
    """åˆ›å»ºå¤šè¯­è¨€ç¿»è¯‘å¯¹"""
    print("ğŸŒ åˆ›å»ºå¤šè¯­è¨€ç¿»è¯‘æ•°æ®...")
    
    # é€‰æ‹©çš„è¯­è¨€
    languages = ['en', 'de', 'es']  # è‹±è¯­ã€å¾·è¯­ã€è¥¿ç­ç‰™è¯­
    lang_names = {
        'en': 'è‹±è¯­',
        'de': 'å¾·è¯­', 
        'es': 'è¥¿ç­ç‰™è¯­'
    }
    
    source_dir = "fairseq/models/ZeroTrans/europarl_scripts/build_data/tokenized"
    output_dir = "multilingual_data"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ“‚ æºç›®å½•: {source_dir}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸŒ é€‰æ‹©è¯­è¨€: {', '.join([f'{lang}({lang_names[lang]})' for lang in languages])}")
    
    # åˆ›å»ºæ‰€æœ‰è¯­è¨€å¯¹
    pairs = []
    for i, src_lang in enumerate(languages):
        for j, tgt_lang in enumerate(languages):
            if i != j:  # ä¸åŒè¯­è¨€ä¹‹é—´
                pairs.append((src_lang, tgt_lang))
    
    print(f"\nğŸ“‹ å°†åˆ›å»º {len(pairs)} ä¸ªç¿»è¯‘æ–¹å‘:")
    for src, tgt in pairs:
        print(f"  {src}({lang_names[src]}) â†’ {tgt}({lang_names[tgt]})")
    
    # å¤åˆ¶å’Œé‡å‘½åæ–‡ä»¶
    for split in ['train', 'valid', 'test']:
        print(f"\nğŸ”„ å¤„ç† {split} æ•°æ®...")
        
        for src_lang, tgt_lang in pairs:
            pair_name = f"{src_lang}-{tgt_lang}"
            
            # æºæ–‡ä»¶è·¯å¾„
            src_file = os.path.join(source_dir, f"{split}.{src_lang}")
            tgt_file = os.path.join(source_dir, f"{split}.{tgt_lang}")
            
            # ç›®æ ‡æ–‡ä»¶è·¯å¾„
            out_src_file = os.path.join(output_dir, f"{split}.{pair_name}.{src_lang}")
            out_tgt_file = os.path.join(output_dir, f"{split}.{pair_name}.{tgt_lang}")
            
            if os.path.exists(src_file) and os.path.exists(tgt_file):
                shutil.copy2(src_file, out_src_file)
                shutil.copy2(tgt_file, out_tgt_file)
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                src_size = os.path.getsize(out_src_file) / (1024*1024)  # MB
                tgt_size = os.path.getsize(out_tgt_file) / (1024*1024)  # MB
                
                print(f"  âœ… {pair_name}: {src_size:.1f}MB + {tgt_size:.1f}MB")
            else:
                print(f"  âŒ {pair_name}: æ–‡ä»¶ä¸å­˜åœ¨")
    
    # åˆ›å»ºé¢„å¤„ç†è„šæœ¬
    create_preprocess_script(languages, output_dir)
    
    print(f"\nâœ… å¤šè¯­è¨€æ•°æ®åˆ›å»ºå®Œæˆ!")
    print(f"ğŸ“ æ•°æ®ä½ç½®: {output_dir}/")
    print(f"ğŸš€ ä¸‹ä¸€æ­¥: è¿è¡Œ python preprocess_multilingual.py")

def create_preprocess_script(languages, output_dir):
    """åˆ›å»ºé¢„å¤„ç†è„šæœ¬"""
    print(f"\nğŸ“ åˆ›å»ºé¢„å¤„ç†è„šæœ¬...")
    
    script_content = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šè¯­è¨€æ•°æ®é¢„å¤„ç†è„šæœ¬
"""

import os
import subprocess

def preprocess_multilingual():
    """é¢„å¤„ç†å¤šè¯­è¨€æ•°æ®"""
    print("ğŸ”„ å¼€å§‹å¤šè¯­è¨€æ•°æ®é¢„å¤„ç†...")
    
    languages = {languages}
    output_dir = "{output_dir}"
    bin_dir = "{output_dir}-bin"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(bin_dir, exist_ok=True)
    
    # æ„å»ºfairseq-preprocesså‘½ä»¤
    cmd = [
        "python", "-m", "fairseq_cli.preprocess",
        "--source-lang", "src",
        "--target-lang", "tgt", 
        "--trainpref", f"{{output_dir}}/train",
        "--validpref", f"{{output_dir}}/valid",
        "--testpref", f"{{output_dir}}/test",
        "--destdir", bin_dir,
        "--workers", "4",
        "--joined-dictionary",  # å…±äº«è¯å…¸
        "--bpe", "subword_nmt",
        "--bpe-codes", "fairseq/models/ZeroTrans/europarl_scripts/build_data/europarl.bpe.model"
    ]
    
    print("ğŸ’» è¿è¡Œå‘½ä»¤:")
    print(" ".join(cmd))
    
    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print("âœ… é¢„å¤„ç†æˆåŠŸ!")
        print(f"ğŸ“ äºŒè¿›åˆ¶æ•°æ®ä¿å­˜åœ¨: {{bin_dir}}/")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ é¢„å¤„ç†å¤±è´¥: {{e}}")
        print(f"é”™è¯¯è¾“å‡º: {{e.stderr}}")
        return False

def create_training_script():
    """åˆ›å»ºå¤šè¯­è¨€è®­ç»ƒè„šæœ¬"""
    print("ğŸ“ åˆ›å»ºå¤šè¯­è¨€è®­ç»ƒè„šæœ¬...")
    
    training_script = """#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import subprocess

def train_multilingual():
    cmd = [
        "python", "-m", "fairseq_cli.train",
        "{output_dir}-bin",
        "--arch", "transformer",
        "--source-lang", "src",
        "--target-lang", "tgt",
        "--lr", "0.0005",
        "--min-lr", "1e-09",
        "--lr-scheduler", "inverse_sqrt",
        "--weight-decay", "0.0001",
        "--criterion", "label_smoothed_cross_entropy",
        "--label-smoothing", "0.1",
        "--max-tokens", "4096",
        "--eval-bleu",
        "--eval-bleu-args", '{{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}}',
        "--eval-bleu-detok", "moses",
        "--eval-bleu-remove-bpe",
        "--best-checkpoint-metric", "bleu",
        "--maximize-best-checkpoint-metric",
        "--save-dir", "pdec_work/checkpoints/multilingual",
        "--tensorboard-logdir", "pdec_work/logs/multilingual",
        "--max-epoch", "3",
        "--patience", "10"
    ]
    
    print("ğŸš€ å¼€å§‹å¤šè¯­è¨€è®­ç»ƒ...")
    subprocess.run(cmd)

if __name__ == "__main__":
    train_multilingual()
"""
    
    with open("train_multilingual.py", "w", encoding="utf-8") as f:
        f.write(training_script)
    
    print("âœ… åˆ›å»ºäº† train_multilingual.py")

if __name__ == "__main__":
    if preprocess_multilingual():
        create_training_script()
        print("\\nğŸ‰ å¤šè¯­è¨€é¢„å¤„ç†å®Œæˆ!")
        print("ğŸš€ ä¸‹ä¸€æ­¥: python train_multilingual.py")
'''
    
    with open("preprocess_multilingual.py", "w", encoding="utf-8") as f:
        f.write(script_content)
    
    print("âœ… åˆ›å»ºäº† preprocess_multilingual.py")

if __name__ == "__main__":
    create_multilingual_pairs() 