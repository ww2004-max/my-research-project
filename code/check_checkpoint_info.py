#!/usr/bin/env python3
import torch
import os

def check_checkpoint(checkpoint_path):
    """æ£€æŸ¥checkpointçš„è¯¦ç»†ä¿¡æ¯"""
    if not os.path.exists(checkpoint_path):
        print(f"âŒ Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return
    
    try:
        print(f"ğŸ“ æ£€æŸ¥checkpoint: {checkpoint_path}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(checkpoint_path) / 1024**2:.1f} MB")
        
        # åŠ è½½checkpoint
        ckpt = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        
        print("\nğŸ“‹ Checkpointä¿¡æ¯:")
        print(f"  - é¡¶çº§é”®: {list(ckpt.keys())}")
        
        # æ£€æŸ¥ä¸åŒå¯èƒ½çš„é”®å
        epoch_keys = ['epoch', 'epochs_done', 'epoch_done']
        for key in epoch_keys:
            if key in ckpt:
                print(f"  - {key}: {ckpt[key]}")
                break
        else:
            print(f"  - Epoch: N/A")
            
        # æ£€æŸ¥æŸå¤±ç›¸å…³çš„é”®
        loss_keys = ['best_loss', 'loss', 'valid_loss', 'train_loss']
        for key in loss_keys:
            if key in ckpt:
                print(f"  - {key}: {ckpt[key]}")
                break
        else:
            print(f"  - Loss: N/A")
            
        # æ£€æŸ¥æ›´æ–°æ­¥æ•°
        update_keys = ['num_updates', 'updates', 'step']
        for key in update_keys:
            if key in ckpt:
                print(f"  - {key}: {ckpt[key]}")
                break
        else:
            print(f"  - Updates: N/A")
        
        # æ£€æŸ¥ä¼˜åŒ–å™¨çŠ¶æ€
        if 'optimizer_history' in ckpt and ckpt['optimizer_history']:
            opt_state = ckpt['optimizer_history'][-1]
            print(f"  - ä¼˜åŒ–å™¨çŠ¶æ€: å­˜åœ¨ ({len(opt_state)} ä¸ªé”®)")
            if 'lr' in opt_state:
                print(f"  - å­¦ä¹ ç‡: {opt_state['lr']}")
        else:
            print(f"  - ä¼˜åŒ–å™¨çŠ¶æ€: ä¸å­˜åœ¨")
            
        # æ£€æŸ¥æ¨¡å‹å‚æ•°
        if 'model' in ckpt:
            model_params = sum(p.numel() for p in ckpt['model'].values() if isinstance(p, torch.Tensor))
            print(f"  - æ¨¡å‹å‚æ•°æ•°é‡: {model_params:,}")
            
        # æ£€æŸ¥extra_stateä¸­çš„è®­ç»ƒä¿¡æ¯
        if 'extra_state' in ckpt and ckpt['extra_state']:
            extra = ckpt['extra_state']
            print(f"  - extra_stateé”®: {list(extra.keys()) if isinstance(extra, dict) else 'Not dict'}")
            if isinstance(extra, dict):
                for key in ['epoch', 'num_updates', 'best_loss', 'train_iterator']:
                    if key in extra:
                        print(f"  - extra_state.{key}: {extra[key]}")
        
        # æ£€æŸ¥task_state
        if 'task_state' in ckpt and ckpt['task_state']:
            task = ckpt['task_state']
            print(f"  - task_state: {task}")
            
        # æ£€æŸ¥argså‚æ•°
        if 'args' in ckpt:
            args = ckpt['args']
            if hasattr(args, 'max_epoch'):
                print(f"  - æœ€å¤§epoch: {args.max_epoch}")
            if hasattr(args, 'lr'):
                print(f"  - åˆå§‹å­¦ä¹ ç‡: {args.lr}")
                
        # æ£€æŸ¥cfgå‚æ•°
        if 'cfg' in ckpt:
            cfg = ckpt['cfg']
            print(f"  - cfgç±»å‹: {type(cfg)}")
            if hasattr(cfg, 'optimization') and hasattr(cfg.optimization, 'lr'):
                print(f"  - cfgå­¦ä¹ ç‡: {cfg.optimization.lr}")
            if hasattr(cfg, 'checkpoint') and hasattr(cfg.checkpoint, 'best_checkpoint_metric'):
                print(f"  - æœ€ä½³æŒ‡æ ‡: {cfg.checkpoint.best_checkpoint_metric}")
        
        print("âœ… Checkpointæ£€æŸ¥å®Œæˆ\n")
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥checkpointæ—¶å‡ºé”™: {e}\n")

if __name__ == "__main__":
    # æ£€æŸ¥æ‰€æœ‰ç›¸å…³çš„checkpoint
    checkpoints = [
        "pdec_work/checkpoints/europarl_test/1/checkpoint_best.pt",
        "pdec_work/checkpoints/europarl_continue_5epochs/checkpoint_best.pt",
        "pdec_work/checkpoints/europarl_continue_5epochs/checkpoint.best_loss_5.4854.pt"
    ]
    
    for ckpt_path in checkpoints:
        check_checkpoint(ckpt_path) 