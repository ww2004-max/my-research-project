#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„å¤šè¯­è¨€æ•°æ®é¢„å¤„ç†
ç›´æ¥ä½¿ç”¨ç°æœ‰çš„BPEå¤„ç†æ–¹å¼
"""

import os
import shutil
import subprocess

def prepare_multilingual_data():
    """å‡†å¤‡å¤šè¯­è¨€æ•°æ®ï¼Œä½¿ç”¨ç°æœ‰çš„BPEæ–¹å¼"""
    print("ğŸŒ å‡†å¤‡å¤šè¯­è¨€ç¿»è¯‘æ•°æ®...")
    
    # æ£€æŸ¥æ•°æ®
    data_dir = "multilingual_data"
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return False
    
    print(f"ğŸ“‚ æ•°æ®ç›®å½•: {data_dir}")
    
    # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
    files = os.listdir(data_dir)
    print(f"ğŸ“‹ æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶:")
    
    # æŒ‰è¯­è¨€å¯¹åˆ†ç»„
    language_pairs = {}
    for file in files:
        if '.' in file:
            parts = file.split('.')
            if len(parts) >= 3:
                split = parts[0]  # train/valid/test
                pair = parts[1]   # en-de, de-esç­‰
                lang = parts[2]   # en, de, es
                
                if pair not in language_pairs:
                    language_pairs[pair] = {}
                if split not in language_pairs[pair]:
                    language_pairs[pair][split] = {}
                
                language_pairs[pair][split][lang] = file
    
    print(f"\nğŸ” å‘ç°çš„è¯­è¨€å¯¹:")
    for pair, splits in language_pairs.items():
        src_lang, tgt_lang = pair.split('-')
        print(f"  {pair}: {src_lang} â†’ {tgt_lang}")
        for split in ['train', 'valid', 'test']:
            if split in splits:
                src_file = splits[split].get(src_lang, 'âŒ')
                tgt_file = splits[split].get(tgt_lang, 'âŒ')
                print(f"    {split}: {src_file} + {tgt_file}")
    
    # åˆ›å»ºåˆå¹¶çš„å¤šè¯­è¨€æ•°æ®
    create_combined_data(data_dir, language_pairs)
    
    return True

def create_combined_data(data_dir, language_pairs):
    """åˆ›å»ºåˆå¹¶çš„å¤šè¯­è¨€æ•°æ®"""
    print(f"\nğŸ”„ åˆ›å»ºåˆå¹¶çš„å¤šè¯­è¨€æ•°æ®...")
    
    output_dir = "multilingual_combined"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¸ºæ¯ä¸ªsplitåˆ›å»ºåˆå¹¶æ–‡ä»¶
    for split in ['train', 'valid', 'test']:
        print(f"\nğŸ“ å¤„ç† {split} æ•°æ®...")
        
        src_combined = os.path.join(output_dir, f"{split}.src")
        tgt_combined = os.path.join(output_dir, f"{split}.tgt")
        
        with open(src_combined, 'w', encoding='utf-8') as src_out, \
             open(tgt_combined, 'w', encoding='utf-8') as tgt_out:
            
            total_lines = 0
            
            for pair, splits in language_pairs.items():
                if split not in splits:
                    continue
                
                src_lang, tgt_lang = pair.split('-')
                
                if src_lang in splits[split] and tgt_lang in splits[split]:
                    src_file = os.path.join(data_dir, splits[split][src_lang])
                    tgt_file = os.path.join(data_dir, splits[split][tgt_lang])
                    
                    if os.path.exists(src_file) and os.path.exists(tgt_file):
                        # è¯»å–å¹¶å†™å…¥æºæ–‡ä»¶
                        with open(src_file, 'r', encoding='utf-8') as f:
                            src_lines = f.readlines()
                        
                        # è¯»å–å¹¶å†™å…¥ç›®æ ‡æ–‡ä»¶
                        with open(tgt_file, 'r', encoding='utf-8') as f:
                            tgt_lines = f.readlines()
                        
                        # ç¡®ä¿è¡Œæ•°åŒ¹é…
                        min_lines = min(len(src_lines), len(tgt_lines))
                        
                        for i in range(min_lines):
                            # æ·»åŠ è¯­è¨€æ ‡è®°
                            src_line = f"<{tgt_lang}> " + src_lines[i].strip() + "\n"
                            tgt_line = tgt_lines[i]
                            
                            src_out.write(src_line)
                            tgt_out.write(tgt_line)
                        
                        total_lines += min_lines
                        print(f"  âœ… {pair}: {min_lines} è¡Œ")
            
            print(f"  ğŸ“Š {split} æ€»è®¡: {total_lines} è¡Œ")
    
    print(f"\nâœ… åˆå¹¶æ•°æ®åˆ›å»ºå®Œæˆ!")
    print(f"ğŸ“ ä½ç½®: {output_dir}/")
    
    # åˆ›å»ºç®€å•çš„è®­ç»ƒè„šæœ¬
    create_simple_training_script(output_dir)

def create_simple_training_script(data_dir):
    """åˆ›å»ºç®€å•çš„è®­ç»ƒè„šæœ¬"""
    print(f"\nğŸ“ åˆ›å»ºè®­ç»ƒè„šæœ¬...")
    
    script_content = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šè¯­è¨€æ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import os
import sys
import subprocess

# æ·»åŠ fairseqè·¯å¾„
sys.path.insert(0, os.path.abspath('fairseq'))

def train_multilingual():
    """è®­ç»ƒå¤šè¯­è¨€æ¨¡å‹"""
    print("ğŸš€ å¼€å§‹å¤šè¯­è¨€æ¨¡å‹è®­ç»ƒ...")
    
    # é¦–å…ˆé¢„å¤„ç†æ•°æ®
    print("ğŸ”„ é¢„å¤„ç†æ•°æ®...")
    
    preprocess_cmd = [
        "python", "fairseq/fairseq_cli/preprocess.py",
        "--source-lang", "src",
        "--target-lang", "tgt",
        "--trainpref", "{data_dir}/train",
        "--validpref", "{data_dir}/valid", 
        "--testpref", "{data_dir}/test",
        "--destdir", "{data_dir}-bin",
        "--workers", "4"
    ]
    
    try:
        subprocess.run(preprocess_cmd, check=True)
        print("âœ… é¢„å¤„ç†æˆåŠŸ!")
    except subprocess.CalledProcessError as e:
        print(f"âŒ é¢„å¤„ç†å¤±è´¥: {{e}}")
        return False
    
    # è®­ç»ƒæ¨¡å‹
    print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
    
    train_cmd = [
        "python", "fairseq/fairseq_cli/train.py",
        "{data_dir}-bin",
        "--arch", "transformer_iwslt_de_en",
        "--source-lang", "src",
        "--target-lang", "tgt",
        "--lr", "0.001",
        "--min-lr", "1e-09",
        "--lr-scheduler", "inverse_sqrt",
        "--weight-decay", "0.0001",
        "--criterion", "label_smoothed_cross_entropy",
        "--label-smoothing", "0.1",
        "--max-tokens", "2048",
        "--eval-bleu",
        "--eval-bleu-args", '{{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}}',
        "--eval-bleu-detok", "moses",
        "--eval-bleu-remove-bpe",
        "--best-checkpoint-metric", "bleu",
        "--maximize-best-checkpoint-metric",
        "--save-dir", "pdec_work/checkpoints/multilingual",
        "--tensorboard-logdir", "pdec_work/logs/multilingual",
        "--max-epoch", "3",
        "--patience", "10",
        "--warmup-updates", "4000",
        "--dropout", "0.3",
        "--attention-dropout", "0.1"
    ]
    
    try:
        subprocess.run(train_cmd, check=True)
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {{e}}")
        return False

if __name__ == "__main__":
    train_multilingual()
'''
    
    with open("train_multilingual_simple.py", "w", encoding="utf-8") as f:
        f.write(script_content)
    
    print("âœ… åˆ›å»ºäº† train_multilingual_simple.py")

if __name__ == "__main__":
    if prepare_multilingual_data():
        print("\nğŸ‰ å¤šè¯­è¨€æ•°æ®å‡†å¤‡å®Œæˆ!")
        print("ğŸš€ ä¸‹ä¸€æ­¥: python train_multilingual_simple.py")
    else:
        print("âŒ æ•°æ®å‡†å¤‡å¤±è´¥") 