#!/usr/bin/env python3
"""
æœ€ç®€å•çš„ç¿»è¯‘è„šæœ¬
ç»•è¿‡å¤æ‚çš„é…ç½®é—®é¢˜
"""

import os
import sys
import torch
import argparse

def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    ROOT_PATH = r"C:\Users\33491\PycharmProjects\machine"
    FAIRSEQ = os.path.join(ROOT_PATH, "fairseq")
    
    os.chdir(ROOT_PATH)
    sys.path.insert(0, FAIRSEQ)
    sys.path.insert(0, os.path.join(FAIRSEQ, "models", "PhasedDecoder"))
    
    # å¯¼å…¥å¿…è¦æ¨¡å—
    import models.transformer_pdec
    import criterions.label_smoothed_cross_entropy_instruction
    
    return ROOT_PATH, FAIRSEQ

def create_simple_args():
    """åˆ›å»ºç®€å•çš„å‚æ•°å¯¹è±¡"""
    args = argparse.Namespace()
    
    # åŸºæœ¬è·¯å¾„
    args.data = "fairseq/models/ZeroTrans/europarl_scripts/build_data/europarl_15-bin"
    args.path = "pdec_work/checkpoints/europarl_test/1/checkpoint_best_fixed.pt"
    
    # ä»»åŠ¡è®¾ç½®
    args.task = 'translation_multi_simple_epoch'
    args.source_lang = 'en'
    args.target_lang = 'de'
    args.lang_pairs = ['en-de', 'de-en', 'en-es', 'es-en', 'en-it', 'it-en']
    args.langs = ['en', 'de', 'es', 'it']
    
    # æ¨¡å‹è®¾ç½®
    args.arch = 'transformer_pdec_6_e_6_d'
    args.encoder_langtok = 'tgt'
    args.decoder_langtok = False
    
    # ç”Ÿæˆè®¾ç½®
    args.beam = 5
    args.max_len_a = 0
    args.max_len_b = 200
    args.min_len = 1
    args.lenpen = 1.0
    args.unkpen = 0.0
    args.temperature = 1.0
    args.remove_bpe = None
    
    # å…¶ä»–è®¾ç½®
    args.cpu = False
    args.fp16 = False
    args.seed = 1
    args.no_progress_bar = True
    args.quiet = False
    
    return args

def simple_translate():
    """ç®€å•ç¿»è¯‘æµ‹è¯•"""
    print("ğŸš€ æœ€ç®€å•ç¿»è¯‘æµ‹è¯•")
    print("=" * 60)
    
    try:
        # è®¾ç½®ç¯å¢ƒ
        ROOT_PATH, FAIRSEQ = setup_environment()
        print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")
        
        # åˆ›å»ºå‚æ•°
        args = create_simple_args()
        print("âœ… å‚æ•°åˆ›å»ºå®Œæˆ")
        
        # å¯¼å…¥fairseqæ¨¡å—
        from fairseq import checkpoint_utils, tasks, utils
        from fairseq.data import Dictionary
        
        print("âœ… fairseqæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ” åŠ è½½æ¨¡å‹: {args.path}")
        models, model_args = checkpoint_utils.load_model_ensemble([args.path])
        model = models[0]
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"âœ… æ¨¡å‹ç±»å‹: {type(model)}")
        
        # æ‰‹åŠ¨åŠ è½½å­—å…¸
        dict_path = os.path.join(args.data, "dict.en.txt")
        if os.path.exists(dict_path):
            src_dict = Dictionary.load(dict_path)
            tgt_dict = src_dict  # å…±äº«å­—å…¸
            print(f"âœ… å­—å…¸åŠ è½½æˆåŠŸï¼Œå¤§å°: {len(src_dict)}")
        else:
            print(f"âŒ å­—å…¸æ–‡ä»¶ä¸å­˜åœ¨: {dict_path}")
            return
        
        # è®¾ç½®è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() and not args.cpu else 'cpu')
        model = model.to(device)
        model.eval()
        
        print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
        
        # æµ‹è¯•ç¿»è¯‘
        test_sentences = [
            "Hello",
            "How are you",
            "Thank you"
        ]
        
        print("\nğŸ”„ å¼€å§‹ç¿»è¯‘æµ‹è¯•...")
        
        for sentence in test_sentences:
            print(f"\nåŸæ–‡: {sentence}")
            
            try:
                # ç®€å•çš„tokenåŒ–
                tokens = src_dict.encode_line(sentence, add_if_not_exist=False)
                print(f"Tokens: {tokens}")
                
                # å‡†å¤‡è¾“å…¥
                src_tokens = tokens.unsqueeze(0).to(device)
                src_lengths = torch.LongTensor([src_tokens.size(1)]).to(device)
                
                print(f"è¾“å…¥å½¢çŠ¶: {src_tokens.shape}")
                
                # ç®€å•çš„å‰å‘ä¼ æ’­æµ‹è¯•
                with torch.no_grad():
                    # ç¼–ç 
                    encoder_out = model.encoder(src_tokens, src_lengths)
                    print(f"ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶: {encoder_out['encoder_out'][0].shape}")
                    
                    # ç®€å•è§£ç ï¼ˆåªå–ç¬¬ä¸€ä¸ªtokenï¼‰
                    prev_output_tokens = torch.LongTensor([[tgt_dict.eos()]]).to(device)
                    decoder_out = model.decoder(prev_output_tokens, encoder_out)
                    
                    # è·å–æ¦‚ç‡æœ€é«˜çš„token
                    probs = torch.softmax(decoder_out[0], dim=-1)
                    next_token = probs.argmax(dim=-1)
                    
                    print(f"ä¸‹ä¸€ä¸ªtoken: {next_token.item()}")
                    print(f"å¯¹åº”è¯æ±‡: {tgt_dict[next_token.item()]}")
                
                print("âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
                
            except Exception as e:
                print(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        print("\nğŸ‰ åŸºæœ¬ç¿»è¯‘æµ‹è¯•å®Œæˆ!")
        print("ğŸ’¡ æ¨¡å‹å¯ä»¥æ­£å¸¸å·¥ä½œï¼Œåªéœ€è¦å®Œå–„ç¿»è¯‘é€»è¾‘")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    simple_translate()

if __name__ == "__main__":
    main() 