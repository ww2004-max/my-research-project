#!/usr/bin/env python3
"""
é«˜çº§ç¿»è¯‘è„šæœ¬ - ä½¿ç”¨å¤šç§è§£ç ç­–ç•¥
"""

import torch
import sentencepiece as spm
import os
import sys

# æ·»åŠ fairseqè·¯å¾„
sys.path.insert(0, 'fairseq')

def load_bpe_model():
    """åŠ è½½BPEæ¨¡å‹"""
    bpe_model_path = "fairseq/models/ZeroTrans/europarl_scripts/build_data/europarl.bpe.model"
    sp = spm.SentencePieceProcessor()
    sp.load(bpe_model_path)
    return sp

def load_fairseq_dict():
    """åŠ è½½fairseqå­—å…¸"""
    from fairseq.data import Dictionary
    dict_path = "fairseq/models/ZeroTrans/europarl_scripts/build_data/europarl_15-bin/dict.txt"
    dictionary = Dictionary.load(dict_path)
    return dictionary

def create_simple_model():
    """åˆ›å»ºç®€å•çš„ç¿»è¯‘æ¨¡å‹ï¼ˆä¸ä¾èµ–å¤æ‚çš„fairseqé…ç½®ï¼‰"""
    print("ğŸ”§ åˆ›å»ºç®€åŒ–æ¨¡å‹...")
    
    # åŠ è½½checkpoint
    checkpoint_path = "pdec_work/checkpoints/europarl_test/1/checkpoint_best_fixed.pt"
    state = torch.load(checkpoint_path, map_location='cpu')
    
    # æå–æ¨¡å‹æƒé‡
    model_state = state['model']
    
    # åˆ›å»ºç®€å•çš„transformeræ¨¡å‹
    class SimpleTransformer(torch.nn.Module):
        def __init__(self, model_state):
            super().__init__()
            self.model_state = model_state
            
            # æå–å…³é”®å‚æ•°
            self.embed_dim = 512  # ä»checkpointåˆ†æå¾—å‡º
            self.vocab_size = 50005
            
            # é‡å»ºåµŒå…¥å±‚
            self.encoder_embed = torch.nn.Embedding(self.vocab_size, self.embed_dim)
            self.decoder_embed = torch.nn.Embedding(self.vocab_size, self.embed_dim)
            
            # è¾“å‡ºæŠ•å½±å±‚
            self.output_projection = torch.nn.Linear(self.embed_dim, self.vocab_size, bias=False)
            
            # åŠ è½½æƒé‡
            self.load_weights()
        
        def load_weights(self):
            """åŠ è½½é¢„è®­ç»ƒæƒé‡"""
            # åŠ è½½åµŒå…¥å±‚æƒé‡
            if 'encoder.embed_tokens.weight' in self.model_state:
                self.encoder_embed.weight.data = self.model_state['encoder.embed_tokens.weight']
            if 'decoder.embed_tokens.weight' in self.model_state:
                self.decoder_embed.weight.data = self.model_state['decoder.embed_tokens.weight']
            
            # åŠ è½½è¾“å‡ºå±‚æƒé‡
            if 'decoder.output_projection.weight' in self.model_state:
                self.output_projection.weight.data = self.model_state['decoder.output_projection.weight']
        
        def simple_forward(self, src_tokens, max_len=50):
            """ç®€å•çš„å‰å‘ä¼ æ’­ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰"""
            batch_size = src_tokens.size(0)
            device = src_tokens.device
            
            # ç®€åŒ–çš„ç¼–ç ï¼ˆä»…ä½¿ç”¨åµŒå…¥ï¼‰
            src_embed = self.encoder_embed(src_tokens)
            # è¿™é‡Œåº”è¯¥æœ‰transformerå±‚ï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨åµŒå…¥çš„å¹³å‡å€¼ä½œä¸ºä¸Šä¸‹æ–‡
            context = src_embed.mean(dim=1, keepdim=True)  # [batch, 1, embed_dim]
            
            # è§£ç 
            tgt_tokens = torch.LongTensor([[2]]).to(device)  # ä»</s>å¼€å§‹
            
            for step in range(max_len):
                tgt_embed = self.decoder_embed(tgt_tokens)
                # ç®€åŒ–çš„è§£ç å™¨ï¼ˆå®é™…åº”è¯¥æœ‰attentionæœºåˆ¶ï¼‰
                decoder_out = tgt_embed + context  # ç®€å•ç›¸åŠ 
                
                # è¾“å‡ºæŠ•å½±
                logits = self.output_projection(decoder_out[:, -1, :])  # æœ€åä¸€ä¸ªä½ç½®
                
                # è·å–ä¸‹ä¸€ä¸ªtoken
                next_token = logits.argmax(dim=-1, keepdim=True)
                tgt_tokens = torch.cat([tgt_tokens, next_token], dim=1)
                
                if next_token.item() == 2:  # </s>
                    break
            
            return tgt_tokens
    
    model = SimpleTransformer(model_state)
    model.eval()
    
    if torch.cuda.is_available():
        model.cuda()
    
    return model

def translate_with_different_strategies(text, src_lang="en", tgt_lang="de"):
    """ä½¿ç”¨ä¸åŒç­–ç•¥ç¿»è¯‘"""
    print(f"ğŸŒ ç¿»è¯‘: '{text}' ({src_lang} -> {tgt_lang})")
    print("=" * 60)
    
    # åŠ è½½BPEæ¨¡å‹
    sp = load_bpe_model()
    
    # ç¼–ç è¾“å…¥
    bpe_pieces = sp.encode_as_pieces(text)
    bpe_ids = sp.encode_as_ids(text)
    
    print(f"ğŸ“ BPEç¼–ç : {bpe_pieces}")
    print(f"ğŸ“ Token IDs: {bpe_ids}")
    
    # è½¬æ¢ä¸ºtensor
    src_tokens = torch.LongTensor([bpe_ids + [2]])  # æ·»åŠ </s>
    if torch.cuda.is_available():
        src_tokens = src_tokens.cuda()
    
    # åŠ è½½æ¨¡å‹
    try:
        model = create_simple_model()
        
        print(f"\nğŸ¯ ç­–ç•¥1: ç®€å•è§£ç ")
        with torch.no_grad():
            output_tokens = model.simple_forward(src_tokens, max_len=30)
            output_ids = output_tokens[0].cpu().tolist()
            
            # è§£ç è¾“å‡º
            # ç§»é™¤ç‰¹æ®Štoken
            clean_ids = [id for id in output_ids if id not in [0, 1, 2, 3]]  # ç§»é™¤pad, bos, eos, unk
            
            if clean_ids:
                output_text = sp.decode_ids(clean_ids)
                print(f"   ç»“æœ: '{output_text}'")
            else:
                print(f"   ç»“æœ: [ç©ºè¾“å‡º]")
            
            print(f"   åŸå§‹IDs: {output_ids}")
        
        # ç­–ç•¥2: éšæœºé‡‡æ ·
        print(f"\nğŸ¯ ç­–ç•¥2: éšæœºé‡‡æ ·è§£ç ")
        with torch.no_grad():
            # è¿™é‡Œå¯ä»¥å®ç°æ¸©åº¦é‡‡æ ·ç­‰æ›´å¤æ‚çš„è§£ç ç­–ç•¥
            # ç”±äºæ¨¡å‹ç»“æ„ç®€åŒ–ï¼Œæˆ‘ä»¬å…ˆç”¨åŸºæœ¬æ–¹æ³•
            pass
        
    except Exception as e:
        print(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_direct_output_analysis():
    """ç›´æ¥åˆ†ææ¨¡å‹è¾“å‡ºåˆ†å¸ƒ"""
    print(f"\nğŸ”¬ ç›´æ¥åˆ†ææ¨¡å‹è¾“å‡º")
    print("=" * 60)
    
    try:
        # åŠ è½½checkpoint
        checkpoint_path = "pdec_work/checkpoints/europarl_test/1/checkpoint_best_fixed.pt"
        state = torch.load(checkpoint_path, map_location='cpu')
        model_state = state['model']
        
        # åˆ†æè¾“å‡ºå±‚æƒé‡
        if 'decoder.output_projection.weight' in model_state:
            output_weights = model_state['decoder.output_projection.weight']
            
            # æ‰¾åˆ°æƒé‡æœ€å¤§çš„å‡ ä¸ªtoken
            print("ğŸ“Š è¾“å‡ºå±‚æƒé‡åˆ†æ:")
            
            # è®¡ç®—æ¯ä¸ªtokençš„æƒé‡èŒƒæ•°
            token_norms = torch.norm(output_weights, dim=1)
            top_tokens = torch.topk(token_norms, 20)
            
            print("æƒé‡èŒƒæ•°æœ€å¤§çš„20ä¸ªtoken:")
            sp = load_bpe_model()
            for i, (norm, token_id) in enumerate(zip(top_tokens.values, top_tokens.indices)):
                try:
                    token_text = sp.id_to_piece(token_id.item())
                    print(f"  {i+1:2d}: Token {token_id.item():5d} '{token_text}' (èŒƒæ•°: {norm.item():.4f})")
                except:
                    print(f"  {i+1:2d}: Token {token_id.item():5d} [æ— æ³•è§£ç ] (èŒƒæ•°: {norm.item():.4f})")
            
            # æ£€æŸ¥ç‰¹å®štokençš„æƒé‡
            special_tokens = [2, 3, 27]  # </s>, <unk>, å¯èƒ½æ˜¯å¥å·
            print(f"\nç‰¹æ®Štokenæƒé‡åˆ†æ:")
            for token_id in special_tokens:
                if token_id < output_weights.shape[0]:
                    weight_norm = torch.norm(output_weights[token_id]).item()
                    try:
                        token_text = sp.id_to_piece(token_id)
                        print(f"  Token {token_id} '{token_text}': æƒé‡èŒƒæ•° {weight_norm:.4f}")
                    except:
                        print(f"  Token {token_id} [æ— æ³•è§£ç ]: æƒé‡èŒƒæ•° {weight_norm:.4f}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")

def main():
    print("ğŸš€ é«˜çº§ç¿»è¯‘æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•å¥å­
    test_sentences = [
        "The European Parliament",
        "We must consider this proposal",
        "This is very important"
    ]
    
    # ç›´æ¥åˆ†ææ¨¡å‹è¾“å‡º
    test_direct_output_analysis()
    
    # ç¿»è¯‘æµ‹è¯•
    for sentence in test_sentences:
        translate_with_different_strategies(sentence)
        print()

if __name__ == "__main__":
    main() 