#!/usr/bin/env python3
"""
PhasedDecoderè®­ç»ƒè„šæœ¬ - ç»“åˆè®ºæ–‡ä¼˜åŒ–æ–¹æ³•
åŸºäºè®ºæ–‡: Improving Language Transfer Capability of Decoder-only Architecture in MNMT
ä¼˜åŒ–æ–¹æ³•:
1. æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ
2. å­¦ä¹ ç‡è°ƒåº¦ä¼˜åŒ–
3. æ•°æ®å¢å¼ºå’Œæ­£åˆ™åŒ–
4. å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥
"""

import os
import sys
import subprocess
import time
import json
import torch
import psutil
from datetime import datetime

def setup_environment():
    """è®¾ç½®ç¯å¢ƒå’Œè·¯å¾„"""
    ROOT_PATH = r"C:\Users\33491\PycharmProjects\machine"
    FAIRSEQ = os.path.join(ROOT_PATH, "fairseq")
    
    os.chdir(ROOT_PATH)
    sys.path.insert(0, FAIRSEQ)
    
    return ROOT_PATH, FAIRSEQ

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total,utilization.gpu', 
                               '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True, check=True)
        
        lines = result.stdout.strip().split('\n')
        for i, line in enumerate(lines):
            used, total, util = line.split(', ')
            print(f"GPU {i}: {used}MB/{total}MB ({float(used)/float(total)*100:.1f}%), åˆ©ç”¨ç‡: {util}%")
            
        return True
    except:
        print("æ— æ³•è·å–GPUä¿¡æ¯")
        return False

def cleanup_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("âœ… GPUç¼“å­˜å·²æ¸…ç†")

def kill_competing_processes():
    """ç»ˆæ­¢å¯èƒ½ç«äº‰GPUçš„è¿›ç¨‹"""
    try:
        # æŸ¥æ‰¾Pythonè®­ç»ƒè¿›ç¨‹
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if proc.info['name'] == 'python.exe' and proc.info['cmdline']:
                    cmdline = ' '.join(proc.info['cmdline'])
                    if 'fairseq' in cmdline and 'train.py' in cmdline and proc.info['pid'] != os.getpid():
                        print(f"å‘ç°ç«äº‰è¿›ç¨‹ PID {proc.info['pid']}: {cmdline[:100]}...")
                        proc.terminate()
                        print(f"âœ… å·²ç»ˆæ­¢è¿›ç¨‹ {proc.info['pid']}")
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
    except Exception as e:
        print(f"æ¸…ç†è¿›ç¨‹æ—¶å‡ºé”™: {e}")

def create_optimized_training_config():
    """åˆ›å»ºä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""
    config = {
        # åŸºç¡€é…ç½®
        "data_bin": "fairseq/models/ZeroTrans/europarl_scripts/build_data/europarl_15-bin",
        "save_dir": "pdec_work/checkpoints/europarl_paper_optimized",
        "restore_file": "pdec_work/checkpoints/europarl_continue_5epochs/checkpoint_best.pt",
        
        # è®ºæ–‡å¯å‘çš„ä¼˜åŒ–é…ç½®
        "max_tokens": 2048,  # å¹³è¡¡å†…å­˜å’Œæ€§èƒ½
        "update_freq": 2,    # å¢åŠ æœ‰æ•ˆbatch size
        "max_epoch": 10,     # æ›´å¤šè½®æ¬¡ä½†æœ‰æ—©åœ
        
        # é˜²è¿‡æ‹Ÿåˆç­–ç•¥
        "dropout": 0.3,      # å¢åŠ dropout
        "attention_dropout": 0.1,
        "activation_dropout": 0.1,
        "weight_decay": 0.01,
        
        # å­¦ä¹ ç‡ä¼˜åŒ–
        "lr": 0.0003,        # é™ä½å­¦ä¹ ç‡
        "lr_scheduler": "inverse_sqrt",
        "warmup_updates": 1000,
        "warmup_init_lr": 1e-07,
        
        # æ—©åœå’ŒéªŒè¯
        "patience": 5,       # æ—©åœè€å¿ƒå€¼
        "validate_interval": 1,
        "save_interval": 1,
        "keep_best_checkpoints": 3,
        
        # å†…å­˜ä¼˜åŒ–
        "checkpoint_activations": True,
        "ddp_backend": "no_c10d",
        "fp16": True,        # ä½¿ç”¨æ··åˆç²¾åº¦
        
        # å¤šè¯­è¨€ä¼˜åŒ–
        "lang_pairs": "en-de,de-en,en-es,es-en,en-it,it-en",
        "sampling_method": "temperature",
        "sampling_temperature": 1.5,  # å¹³è¡¡ä¸åŒè¯­è¨€å¯¹
        
        # æ­£åˆ™åŒ–
        "label_smoothing": 0.1,
        "clip_norm": 1.0,
    }
    
    return config

def build_training_command(config):
    """æ„å»ºè®­ç»ƒå‘½ä»¤"""
    cmd = [
        "python", "fairseq_cli/train.py",
        config["data_bin"],
        
        # ä»»åŠ¡å’Œæ¶æ„
        "--task", "translation_multi_simple_epoch",
        "--arch", "phaseddecoder_iwslt_de_en",
        "--lang-pairs", config["lang_pairs"],
        
        # æ•°æ®å’Œæ‰¹æ¬¡
        "--max-tokens", str(config["max_tokens"]),
        "--update-freq", str(config["update_freq"]),
        "--max-epoch", str(config["max_epoch"]),
        
        # ä¼˜åŒ–å™¨
        "--optimizer", "adam",
        "--adam-betas", "(0.9, 0.98)",
        "--lr", str(config["lr"]),
        "--lr-scheduler", config["lr_scheduler"],
        "--warmup-updates", str(config["warmup_updates"]),
        "--warmup-init-lr", str(config["warmup_init_lr"]),
        "--weight-decay", str(config["weight_decay"]),
        "--clip-norm", str(config["clip_norm"]),
        
        # æ­£åˆ™åŒ–
        "--dropout", str(config["dropout"]),
        "--attention-dropout", str(config["attention_dropout"]),
        "--activation-dropout", str(config["activation_dropout"]),
        "--label-smoothing", str(config["label_smoothing"]),
        
        # ä¿å­˜å’ŒéªŒè¯
        "--save-dir", config["save_dir"],
        "--restore-file", config["restore_file"],
        "--validate-interval", str(config["validate_interval"]),
        "--save-interval", str(config["save_interval"]),
        "--keep-best-checkpoints", str(config["keep_best_checkpoints"]),
        "--patience", str(config["patience"]),
        "--no-epoch-checkpoints",
        
        # å¤šè¯­è¨€é‡‡æ ·
        "--sampling-method", config["sampling_method"],
        "--sampling-temperature", str(config["sampling_temperature"]),
        
        # å†…å­˜å’Œæ€§èƒ½ä¼˜åŒ–
        "--checkpoint-activations",
        "--ddp-backend", config["ddp_backend"],
        "--fp16",
        "--fp16-init-scale", "128",
        "--fp16-scale-window", "128",
        
        # æ—¥å¿—
        "--log-format", "simple",
        "--log-interval", "50",
        "--tensorboard-logdir", f"{config['save_dir']}/tensorboard",
        
        # å…¶ä»–
        "--seed", "42",
        "--num-workers", "0",
    ]
    
    return cmd

def monitor_training_progress(save_dir):
    """ç›‘æ§è®­ç»ƒè¿›åº¦"""
    log_file = os.path.join(save_dir, "train.log")
    best_loss = float('inf')
    
    print(f"\nğŸ“Š ç›‘æ§è®­ç»ƒè¿›åº¦...")
    print(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    print("=" * 80)
    
    return best_loss

def main():
    print("ğŸš€ PhasedDecoderä¼˜åŒ–è®­ç»ƒ - åŸºäºè®ºæ–‡æ–¹æ³•")
    print("=" * 80)
    print("ä¼˜åŒ–ç­–ç•¥:")
    print("1. æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ")
    print("2. æ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦")
    print("3. å¢å¼ºçš„æ­£åˆ™åŒ–")
    print("4. å¤šè¯­è¨€é‡‡æ ·å¹³è¡¡")
    print("5. æ··åˆç²¾åº¦è®­ç»ƒ")
    print("=" * 80)
    
    try:
        ROOT_PATH, FAIRSEQ = setup_environment()
        print(f"âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ: {ROOT_PATH}")
    except Exception as e:
        print(f"âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥: {e}")
        return
    
    # æ¸…ç†èµ„æº
    print("\nğŸ§¹ æ¸…ç†ç³»ç»Ÿèµ„æº...")
    kill_competing_processes()
    cleanup_gpu_memory()
    
    # æ£€æŸ¥GPUçŠ¶æ€
    print("\nğŸ” æ£€æŸ¥GPUçŠ¶æ€:")
    check_gpu_memory()
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    config = create_optimized_training_config()
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(config["save_dir"], exist_ok=True)
    
    # ä¿å­˜é…ç½®
    config_file = os.path.join(config["save_dir"], "training_config.json")
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“‹ è®­ç»ƒé…ç½®å·²ä¿å­˜: {config_file}")
    
    # æ„å»ºè®­ç»ƒå‘½ä»¤
    cmd = build_training_command(config)
    
    print(f"\nğŸ¯ å¼€å§‹ä¼˜åŒ–è®­ç»ƒ...")
    print(f"ä¿å­˜ç›®å½•: {config['save_dir']}")
    print(f"åŸºç¡€æ¨¡å‹: {config['restore_file']}")
    print(f"æœ€å¤§è½®æ•°: {config['max_epoch']} (æ—©åœè€å¿ƒå€¼: {config['patience']})")
    print(f"å­¦ä¹ ç‡: {config['lr']} (é¢„çƒ­: {config['warmup_updates']}æ­¥)")
    
    # æ˜¾ç¤ºå®Œæ•´å‘½ä»¤
    print(f"\nğŸ“ è®­ç»ƒå‘½ä»¤:")
    print(" ".join(cmd))
    
    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    
    try:
        # é‡å®šå‘è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶
        log_file = os.path.join(config["save_dir"], "train.log")
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(f"è®­ç»ƒå¼€å§‹æ—¶é—´: {datetime.now()}\n")
            f.write(f"è®­ç»ƒå‘½ä»¤: {' '.join(cmd)}\n")
            f.write("=" * 80 + "\n")
        
        print(f"\nâ° è®­ç»ƒå¼€å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“„ æ—¥å¿—æ–‡ä»¶: {log_file}")
        print("ğŸ”„ è®­ç»ƒè¿›è¡Œä¸­...")
        
        # å¯åŠ¨è®­ç»ƒè¿›ç¨‹
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=1
        )
        
        # å®æ—¶æ˜¾ç¤ºè¾“å‡º
        with open(log_file, 'a', encoding='utf-8') as f:
            for line in process.stdout:
                print(line.rstrip())
                f.write(line)
                f.flush()
        
        process.wait()
        
        if process.returncode == 0:
            end_time = time.time()
            duration = end_time - start_time
            print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
            print(f"â±ï¸  æ€»ç”¨æ—¶: {duration/3600:.2f}å°æ—¶")
            print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {config['save_dir']}")
            
            # æ˜¾ç¤ºæœ€ç»ˆGPUçŠ¶æ€
            print(f"\nğŸ” æœ€ç»ˆGPUçŠ¶æ€:")
            check_gpu_memory()
            
        else:
            print(f"\nâŒ è®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºç : {process.returncode}")
            
    except KeyboardInterrupt:
        print(f"\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        if 'process' in locals():
            process.terminate()
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    finally:
        cleanup_gpu_memory()

if __name__ == "__main__":
    main() 