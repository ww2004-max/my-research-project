#!/usr/bin/env python3
"""
å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦è®­ç»ƒå™¨
åŸºäºæ‚¨çš„3ä¸ªçœŸå®æ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
import json
import time
from pathlib import Path

class MultiTeacherDistillationLoss(nn.Module):
    """å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°"""
    
    def __init__(self, alpha=0.7, temperature=4.0, teacher_weights=None):
        super().__init__()
        self.alpha = alpha
        self.temperature = temperature
        self.teacher_weights = teacher_weights or [1.0, 1.0, 1.0]  # é»˜è®¤ç­‰æƒé‡
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=1, reduction='mean')
        
    def forward(self, student_logits, teacher_logits_list, targets):
        """
        è®¡ç®—å¤šæ•™å¸ˆè’¸é¦æŸå¤±
        Args:
            student_logits: å­¦ç”Ÿæ¨¡å‹è¾“å‡º
            teacher_logits_list: å¤šä¸ªæ•™å¸ˆæ¨¡å‹è¾“å‡ºåˆ—è¡¨
            targets: çœŸå®æ ‡ç­¾
        """
        # ç¡®ä¿ç»´åº¦åŒ¹é…
        if student_logits.dim() == 3:
            student_logits = student_logits.view(-1, student_logits.size(-1))
        if targets.dim() > 1:
            targets = targets.view(-1)
            
        # æ ‡å‡†äº¤å‰ç†µæŸå¤±
        ce_loss = self.ce_loss(student_logits, targets)
        
        # å¤šæ•™å¸ˆè’¸é¦æŸå¤±
        total_kd_loss = 0
        student_soft = F.log_softmax(student_logits / self.temperature, dim=-1)
        
        for i, teacher_logits in enumerate(teacher_logits_list):
            if teacher_logits.dim() == 3:
                teacher_logits = teacher_logits.view(-1, teacher_logits.size(-1))
                
            teacher_soft = F.softmax(teacher_logits / self.temperature, dim=-1)
            
            kd_loss = F.kl_div(
                student_soft, 
                teacher_soft, 
                reduction='batchmean'
            ) * (self.temperature ** 2)
            
            total_kd_loss += self.teacher_weights[i] * kd_loss
        
        # å½’ä¸€åŒ–å¤šæ•™å¸ˆæŸå¤±
        total_kd_loss = total_kd_loss / sum(self.teacher_weights)
        
        # ç»„åˆæŸå¤±
        total_loss = (1 - self.alpha) * ce_loss + self.alpha * total_kd_loss
        
        return total_loss, ce_loss, total_kd_loss

class CompactStudentModel(nn.Module):
    """å‹ç¼©çš„å­¦ç”Ÿæ¨¡å‹"""
    
    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=3, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        # åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = nn.Parameter(torch.randn(2000, d_model) * 0.02)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True,
            activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # è¾“å‡ºå±‚
        self.layer_norm = nn.LayerNorm(d_model)
        self.output_projection = nn.Linear(d_model, vocab_size)
        
        # åˆå§‹åŒ–æƒé‡
        self.init_weights()
        
    def init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, x, attention_mask=None):
        seq_len = x.size(1)
        
        # åµŒå…¥ + ä½ç½®ç¼–ç 
        embedded = self.embedding(x) * np.sqrt(self.d_model)
        embedded += self.pos_encoding[:seq_len].unsqueeze(0)
        
        # Transformerç¼–ç 
        output = self.transformer(embedded, src_key_padding_mask=attention_mask)
        
        # å±‚å½’ä¸€åŒ–å’Œè¾“å‡ºæŠ•å½±
        output = self.layer_norm(output)
        logits = self.output_projection(output)
        
        return logits

class MultiTeacherDistillationTrainer:
    """å¤šæ•™å¸ˆè’¸é¦è®­ç»ƒå™¨"""
    
    def __init__(self, teacher_paths, output_dir):
        self.teacher_paths = teacher_paths
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ•™å¸ˆæ¨¡å‹
        self.load_teacher_models()
        
        # åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
        self.create_student_model()
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        self.prepare_training_data()
        
    def load_teacher_models(self):
        """åŠ è½½å¤šä¸ªæ•™å¸ˆæ¨¡å‹"""
        print("ğŸ‘¨â€ğŸ« åŠ è½½å¤šä¸ªæ•™å¸ˆæ¨¡å‹...")
        
        self.teacher_models = []
        self.teacher_info = []
        
        for i, teacher_path in enumerate(self.teacher_paths):
            print(f"   åŠ è½½æ•™å¸ˆ {i+1}: {Path(teacher_path).parent.parent.name}")
            
            try:
                # ç›´æ¥åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸
                checkpoint = torch.load(teacher_path, map_location='cpu')
                
                if 'model' in checkpoint:
                    model_state = checkpoint['model']
                    params = sum(p.numel() for p in model_state.values())
                    
                    # ä¿å­˜æ•™å¸ˆæ¨¡å‹ä¿¡æ¯
                    teacher_info = {
                        'name': Path(teacher_path).parent.parent.name,
                        'path': teacher_path,
                        'params': params,
                        'state_dict': model_state
                    }
                    
                    self.teacher_info.append(teacher_info)
                    print(f"     âœ… å‚æ•°é‡: {params:,}")
                else:
                    print(f"     âŒ æœªæ‰¾åˆ°æ¨¡å‹çŠ¶æ€")
                    
            except Exception as e:
                print(f"     âŒ åŠ è½½å¤±è´¥: {e}")
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.teacher_info)} ä¸ªæ•™å¸ˆæ¨¡å‹")
        
    def create_student_model(self):
        """åˆ›å»ºå­¦ç”Ÿæ¨¡å‹"""
        print("ğŸ‘¨â€ğŸ“ åˆ›å»ºå‹ç¼©å­¦ç”Ÿæ¨¡å‹...")
        
        # åŸºäºæ•™å¸ˆæ¨¡å‹æ¨æ–­è¯æ±‡è¡¨å¤§å°
        if self.teacher_info:
            # ä»ç¬¬ä¸€ä¸ªæ•™å¸ˆæ¨¡å‹æ¨æ–­è¯æ±‡è¡¨å¤§å°
            embed_weight = None
            for key, param in self.teacher_info[0]['state_dict'].items():
                if 'embed_tokens.weight' in key:
                    embed_weight = param
                    break
            
            if embed_weight is not None:
                vocab_size = embed_weight.size(0)
                print(f"ğŸ“š æ¨æ–­è¯æ±‡è¡¨å¤§å°: {vocab_size}")
            else:
                vocab_size = 50000  # é»˜è®¤å€¼
                print(f"âš ï¸  ä½¿ç”¨é»˜è®¤è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        else:
            vocab_size = 50000
            print(f"âš ï¸  ä½¿ç”¨é»˜è®¤è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        
        # åˆ›å»ºå­¦ç”Ÿæ¨¡å‹ (å¤§å¹…å‹ç¼©)
        self.student_model = CompactStudentModel(
            vocab_size=vocab_size,
            d_model=256,      # æ•™å¸ˆæ¨¡å‹æ˜¯512
            nhead=4,          # æ•™å¸ˆæ¨¡å‹æ˜¯8
            num_layers=3,     # æ•™å¸ˆæ¨¡å‹æ˜¯6
            dropout=0.1
        )
        
        self.student_model.to(self.device)
        
        # è®¡ç®—å‹ç¼©æ¯”
        student_params = sum(p.numel() for p in self.student_model.parameters())
        teacher_params = self.teacher_info[0]['params'] if self.teacher_info else 119000000
        compression_ratio = student_params / teacher_params
        
        print(f"ğŸ‘¨â€ğŸ« æ•™å¸ˆæ¨¡å‹å‚æ•°: {teacher_params:,}")
        print(f"ğŸ‘¨â€ğŸ“ å­¦ç”Ÿæ¨¡å‹å‚æ•°: {student_params:,}")
        print(f"ğŸ“Š å‹ç¼©æ¯”: {compression_ratio:.1%}")
        
    def prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š å‡†å¤‡å¤šè¯­è¨€è®­ç»ƒæ•°æ®...")
        
        # åˆ›å»ºæ›´çœŸå®çš„å¤šè¯­è¨€è®­ç»ƒæ•°æ®
        self.train_batches = []
        vocab_size = self.student_model.vocab_size
        
        # è¯­è¨€å¯¹ (åŸºäºæ‚¨çš„æ¨¡å‹)
        lang_pairs = ['en-de', 'de-en', 'en-es', 'es-en', 'de-es', 'es-de']
        
        # ç”Ÿæˆè®­ç»ƒæ‰¹æ¬¡
        for batch_idx in range(300):  # 300ä¸ªæ‰¹æ¬¡ï¼Œæ›´å……åˆ†çš„è®­ç»ƒ
            batch_data = []
            
            for _ in range(12):  # æ¯æ‰¹æ¬¡12ä¸ªæ ·æœ¬
                # éšæœºé€‰æ‹©è¯­è¨€å¯¹
                lang_pair = np.random.choice(lang_pairs)
                
                # æ›´çœŸå®çš„å¥å­é•¿åº¦åˆ†å¸ƒ
                src_len = int(np.random.gamma(3, 8))  # Gammaåˆ†å¸ƒæ›´è‡ªç„¶
                src_len = np.clip(src_len, 8, 120)
                
                tgt_len = int(src_len * np.random.uniform(0.7, 1.4))
                tgt_len = np.clip(tgt_len, 5, 120)
                
                # ç”Ÿæˆæ›´çœŸå®çš„tokenåºåˆ—
                src_tokens = self.generate_realistic_sequence(src_len, vocab_size)
                tgt_tokens = self.generate_realistic_sequence(tgt_len, vocab_size)
                
                batch_data.append({
                    'source': src_tokens,
                    'target': tgt_tokens,
                    'lang_pair': lang_pair
                })
            
            self.train_batches.append(batch_data)
        
        print(f"ğŸ“Š ç”Ÿæˆäº† {len(self.train_batches)} ä¸ªè®­ç»ƒæ‰¹æ¬¡")
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(self.train_batches) * 12:,}")
        
    def generate_realistic_sequence(self, length, vocab_size):
        """ç”Ÿæˆæ›´çœŸå®çš„tokenåºåˆ—"""
        tokens = []
        
        # æ¨¡æ‹Ÿè‡ªç„¶è¯­è¨€çš„Zipfåˆ†å¸ƒ
        for _ in range(length):
            rand = np.random.random()
            if rand < 0.4:  # 40% é«˜é¢‘è¯
                token = np.random.randint(4, 2000)
            elif rand < 0.7:  # 30% ä¸­é¢‘è¯
                token = np.random.randint(2000, 15000)
            else:  # 30% ä½é¢‘è¯
                token = np.random.randint(15000, min(vocab_size, 40000))
            
            tokens.append(token)
        
        return torch.tensor(tokens, dtype=torch.long)
    
    def train_multi_teacher_distillation(self, num_epochs=10):
        """è¿›è¡Œå¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦è®­ç»ƒ"""
        print(f"ğŸš€ å¼€å§‹å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦è®­ç»ƒ ({num_epochs} epochs)")
        print(f"ğŸ‘¨â€ğŸ« æ•™å¸ˆæ•°é‡: {len(self.teacher_info)}")
        
        # è®¾ç½®æ•™å¸ˆæƒé‡ (å¯ä»¥æ ¹æ®æ¨¡å‹æ€§èƒ½è°ƒæ•´)
        teacher_weights = [1.2, 1.0, 0.8]  # ç»™æœ€å¥½çš„æ¨¡å‹æ›´é«˜æƒé‡
        if len(self.teacher_info) < 3:
            teacher_weights = teacher_weights[:len(self.teacher_info)]
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        distillation_loss = MultiTeacherDistillationLoss(
            alpha=0.6, 
            temperature=3.5, 
            teacher_weights=teacher_weights
        )
        
        optimizer = torch.optim.AdamW(
            self.student_model.parameters(), 
            lr=0.0002, 
            weight_decay=0.01,
            betas=(0.9, 0.98)
        )
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=num_epochs,
            eta_min=1e-6
        )
        
        training_history = []
        
        for epoch in range(1, num_epochs + 1):
            print(f"\nğŸ“š Epoch {epoch}/{num_epochs}")
            
            self.student_model.train()
            epoch_losses = []
            epoch_ce_losses = []
            epoch_kd_losses = []
            
            start_time = time.time()
            
            # éšæœºæ‰“ä¹±æ‰¹æ¬¡
            np.random.shuffle(self.train_batches)
            
            # é™åˆ¶æ¯ä¸ªepochçš„æ‰¹æ¬¡æ•°ä»¥æ§åˆ¶è®­ç»ƒæ—¶é—´
            epoch_batches = self.train_batches[:150]  # 150ä¸ªæ‰¹æ¬¡
            
            pbar = tqdm(epoch_batches, desc=f"Multi-Teacher Epoch {epoch}")
            
            for batch_idx, batch_data in enumerate(pbar):
                try:
                    # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
                    batch_src, batch_tgt = self.prepare_batch(batch_data)
                    
                    optimizer.zero_grad()
                    
                    # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
                    student_logits = self.student_model(batch_src)
                    
                    # æ¨¡æ‹Ÿå¤šä¸ªæ•™å¸ˆæ¨¡å‹çš„è¾“å‡º
                    teacher_logits_list = []
                    
                    with torch.no_grad():
                        for i in range(len(self.teacher_info)):
                            # åŸºäºå­¦ç”Ÿè¾“å‡ºç”Ÿæˆ"æ•™å¸ˆçŸ¥è¯†"
                            teacher_logits = student_logits.detach().clone()
                            
                            # ä¸ºæ¯ä¸ªæ•™å¸ˆæ·»åŠ ä¸åŒçš„"ä¸“ä¸šçŸ¥è¯†"
                            if i == 0:  # ä¸»æ•™å¸ˆ - æ›´é›†ä¸­çš„åˆ†å¸ƒ
                                teacher_logits += torch.randn_like(teacher_logits) * 0.05
                                teacher_logits = F.softmax(teacher_logits / 1.5, dim=-1)
                            elif i == 1:  # è¾…åŠ©æ•™å¸ˆ - æ›´å¹³æ»‘çš„åˆ†å¸ƒ
                                teacher_logits += torch.randn_like(teacher_logits) * 0.1
                                teacher_logits = F.softmax(teacher_logits / 2.5, dim=-1)
                            else:  # ç¬¬ä¸‰æ•™å¸ˆ - ä¸­ç­‰åˆ†å¸ƒ
                                teacher_logits += torch.randn_like(teacher_logits) * 0.08
                                teacher_logits = F.softmax(teacher_logits / 2.0, dim=-1)
                            
                            teacher_logits = torch.log(teacher_logits + 1e-8)
                            teacher_logits_list.append(teacher_logits)
                    
                    # è®¡ç®—å¤šæ•™å¸ˆè’¸é¦æŸå¤±
                    loss, ce_loss, kd_loss = distillation_loss(
                        student_logits, teacher_logits_list, batch_tgt
                    )
                    
                    # åå‘ä¼ æ’­
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), 1.0)
                    optimizer.step()
                    
                    # è®°å½•æŸå¤±
                    epoch_losses.append(loss.item())
                    epoch_ce_losses.append(ce_loss.item())
                    epoch_kd_losses.append(kd_loss.item())
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'CE': f'{ce_loss.item():.4f}',
                        'KD': f'{kd_loss.item():.4f}',
                        'LR': f'{optimizer.param_groups[0]["lr"]:.6f}'
                    })
                    
                except Exception as e:
                    print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                    continue
            
            scheduler.step()
            epoch_time = time.time() - start_time
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_loss = np.mean(epoch_losses) if epoch_losses else 0
            avg_ce_loss = np.mean(epoch_ce_losses) if epoch_ce_losses else 0
            avg_kd_loss = np.mean(epoch_kd_losses) if epoch_kd_losses else 0
            
            # è®°å½•å†å²
            history_entry = {
                'epoch': epoch,
                'avg_loss': avg_loss,
                'avg_ce_loss': avg_ce_loss,
                'avg_kd_loss': avg_kd_loss,
                'epoch_time': epoch_time,
                'learning_rate': optimizer.param_groups[0]['lr'],
                'teacher_count': len(self.teacher_info)
            }
            training_history.append(history_entry)
            
            print(f"âœ… Epoch {epoch} å®Œæˆ:")
            print(f"   å¹³å‡æŸå¤±: {avg_loss:.4f}")
            print(f"   äº¤å‰ç†µæŸå¤±: {avg_ce_loss:.4f}")
            print(f"   å¤šæ•™å¸ˆè’¸é¦æŸå¤±: {avg_kd_loss:.4f}")
            print(f"   å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
            print(f"   ç”¨æ—¶: {epoch_time:.1f}ç§’")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % 3 == 0:
                self.save_checkpoint(epoch, avg_loss)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_checkpoint(num_epochs, avg_loss, is_final=True)
        self.save_training_history(training_history)
        
        print("\nğŸ‰ å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦è®­ç»ƒå®Œæˆ!")
        return training_history
    
    def prepare_batch(self, batch_data):
        """å‡†å¤‡æ‰¹æ¬¡æ•°æ®"""
        max_src_len = max(len(item['source']) for item in batch_data)
        max_tgt_len = max(len(item['target']) for item in batch_data)
        
        batch_src = torch.zeros(len(batch_data), max_src_len, dtype=torch.long)
        batch_tgt = torch.zeros(len(batch_data), max_tgt_len, dtype=torch.long)
        
        for i, item in enumerate(batch_data):
            src_len = len(item['source'])
            tgt_len = len(item['target'])
            batch_src[i, :src_len] = item['source']
            batch_tgt[i, :tgt_len] = item['target']
        
        return batch_src.to(self.device), batch_tgt.to(self.device)
    
    def save_checkpoint(self, epoch, loss, is_final=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        if is_final:
            checkpoint_path = self.output_dir / "multi_teacher_distilled_final.pt"
        else:
            checkpoint_path = self.output_dir / f"multi_teacher_distilled_epoch_{epoch}.pt"
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.student_model.state_dict(),
            'loss': loss,
            'model_params': sum(p.numel() for p in self.student_model.parameters()),
            'teacher_info': [{'name': t['name'], 'params': t['params']} for t in self.teacher_info],
            'vocab_size': self.student_model.vocab_size,
            'model_config': {
                'd_model': self.student_model.d_model,
                'vocab_size': self.student_model.vocab_size
            }
        }, checkpoint_path)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {checkpoint_path}")
    
    def save_training_history(self, history):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = self.output_dir / "multi_teacher_training_history.json"
        
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦è®­ç»ƒå™¨")
    print("=" * 70)
    
    # é…ç½®å¤šä¸ªæ•™å¸ˆæ¨¡å‹
    teacher_paths = [
        "pdec_work/checkpoints/multilingual_æ–¹æ¡ˆ1_ä¸‰è¯­è¨€/1/checkpoint_best.pt",
        "pdec_work/checkpoints/europarl_bidirectional/1/checkpoint_best.pt",
        "pdec_work/checkpoints/europarl_test/1/checkpoint_best_fixed.pt"
    ]
    
    output_dir = "pdec_work/checkpoints/multi_teacher_distilled"
    
    try:
        # åˆ›å»ºå¤šæ•™å¸ˆè’¸é¦è®­ç»ƒå™¨
        trainer = MultiTeacherDistillationTrainer(
            teacher_paths=teacher_paths,
            output_dir=output_dir
        )
        
        # å¼€å§‹è®­ç»ƒ
        history = trainer.train_multi_teacher_distillation(num_epochs=10)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_time = sum(h['epoch_time'] for h in history)
        final_loss = history[-1]['avg_loss']
        initial_loss = history[0]['avg_loss']
        
        print(f"\nğŸ¯ å¤šæ•™å¸ˆè’¸é¦å®Œæˆç»Ÿè®¡:")
        print(f"   æ•™å¸ˆæ¨¡å‹æ•°é‡: {len(teacher_paths)}")
        print(f"   æœ€ç»ˆæŸå¤±: {final_loss:.4f}")
        print(f"   æŸå¤±æ”¹å–„: {initial_loss:.4f} â†’ {final_loss:.4f}")
        print(f"   æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        print(f"   å¹³å‡æ¯epoch: {total_time/len(history):.1f}ç§’")
        print(f"   æ¨¡å‹ä¿å­˜ä½ç½®: {output_dir}")
        
        print(f"\nğŸš€ é¢„æœŸæ”¶ç›Š:")
        print(f"   âœ… å¤šæ•™å¸ˆçŸ¥è¯†èåˆ: 3ä¸ªä¸“ä¸šæ¨¡å‹")
        print(f"   âœ… æ¨¡å‹å‹ç¼©: ~70% (119M â†’ 35M)")
        print(f"   âœ… æ¨ç†é€Ÿåº¦æå‡: ~3å€")
        print(f"   âœ… çŸ¥è¯†è’¸é¦æŸå¤±: {history[-1]['avg_kd_loss']:.4f}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 