#!/usr/bin/env python3
"""
çœŸæ­£çš„çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨
åŸºäºæ‚¨ç°æœ‰çš„å¤šè¯­è¨€æ¨¡å‹è¿›è¡Œè’¸é¦ä¼˜åŒ–
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import json
import time
from pathlib import Path

# æ·»åŠ fairseqè·¯å¾„
sys.path.insert(0, 'fairseq')

try:
    from fairseq.models.transformer import TransformerModel
    from fairseq import checkpoint_utils, utils, tasks
    from fairseq.data import Dictionary
    from fairseq.models import register_model, register_model_architecture
    from fairseq.models.transformer import TransformerEncoder, TransformerDecoder
    print("âœ… Fairseqå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ Fairseqå¯¼å…¥å¤±è´¥: {e}")
    print("ğŸ’¡ è¯·ç¡®ä¿fairseqå·²æ­£ç¡®å®‰è£…")
    sys.exit(1)

class KnowledgeDistillationLoss(nn.Module):
    """çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°"""
    
    def __init__(self, alpha=0.7, temperature=4.0):
        super().__init__()
        self.alpha = alpha  # è’¸é¦æŸå¤±æƒé‡
        self.temperature = temperature  # æ¸©åº¦å‚æ•°
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=1)  # æ ‡å‡†äº¤å‰ç†µ
        
    def forward(self, student_logits, teacher_logits, targets):
        """
        è®¡ç®—çŸ¥è¯†è’¸é¦æŸå¤±
        """
        # æ ‡å‡†äº¤å‰ç†µæŸå¤±
        ce_loss = self.ce_loss(student_logits, targets)
        
        # è’¸é¦æŸå¤± (KLæ•£åº¦)
        student_soft = F.log_softmax(student_logits / self.temperature, dim=-1)
        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=-1)
        
        kd_loss = F.kl_div(
            student_soft, 
            teacher_soft, 
            reduction='batchmean'
        ) * (self.temperature ** 2)
        
        # ç»„åˆæŸå¤±
        total_loss = (1 - self.alpha) * ce_loss + self.alpha * kd_loss
        
        return total_loss, ce_loss, kd_loss

class CompactTransformerModel(nn.Module):
    """å‹ç¼©ç‰ˆçš„Transformerå­¦ç”Ÿæ¨¡å‹"""
    
    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=3, dim_feedforward=1024):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = nn.Parameter(torch.randn(5000, d_model))
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        self.output_projection = nn.Linear(d_model, vocab_size)
        
    def forward(self, src, src_mask=None):
        # åµŒå…¥ + ä½ç½®ç¼–ç 
        seq_len = src.size(1)
        embedded = self.embedding(src) * np.sqrt(self.d_model)
        embedded += self.pos_encoding[:seq_len, :].unsqueeze(0)
        
        # Transformerç¼–ç 
        output = self.transformer(embedded, src_mask)
        
        # è¾“å‡ºæŠ•å½±
        logits = self.output_projection(output)
        return logits

class KnowledgeDistillationTrainer:
    """çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨"""
    
    def __init__(self, teacher_model_path, data_path, output_dir):
        self.teacher_model_path = teacher_model_path
        self.data_path = data_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®
        self.setup_models()
        self.setup_data()
        
    def setup_models(self):
        """è®¾ç½®æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹"""
        print("ğŸ‘¨â€ğŸ« åŠ è½½æ•™å¸ˆæ¨¡å‹...")
        
        try:
            # åŠ è½½æ•™å¸ˆæ¨¡å‹
            self.teacher_model = TransformerModel.from_pretrained(
                model_name_or_path=str(Path(self.teacher_model_path).parent),
                checkpoint_file=Path(self.teacher_model_path).name,
                data_name_or_path=self.data_path
            )
            self.teacher_model.eval()
            self.teacher_model.to(self.device)
            
            # è·å–è¯æ±‡è¡¨å¤§å°
            vocab_size = len(self.teacher_model.task.source_dictionary)
            print(f"ğŸ“š è¯æ±‡è¡¨å¤§å°: {vocab_size}")
            
            # åˆ›å»ºå­¦ç”Ÿæ¨¡å‹ (å‹ç¼©ç‰ˆ)
            print("ğŸ‘¨â€ğŸ“ åˆ›å»ºå­¦ç”Ÿæ¨¡å‹...")
            self.student_model = CompactTransformerModel(
                vocab_size=vocab_size,
                d_model=256,  # åŸæ¨¡å‹å¯èƒ½æ˜¯512
                nhead=4,      # åŸæ¨¡å‹å¯èƒ½æ˜¯8
                num_layers=3, # åŸæ¨¡å‹å¯èƒ½æ˜¯6
                dim_feedforward=1024  # åŸæ¨¡å‹å¯èƒ½æ˜¯2048
            )
            self.student_model.to(self.device)
            
            # è®¡ç®—å‚æ•°é‡
            teacher_params = sum(p.numel() for p in self.teacher_model.parameters())
            student_params = sum(p.numel() for p in self.student_model.parameters())
            compression_ratio = student_params / teacher_params
            
            print(f"ğŸ‘¨â€ğŸ« æ•™å¸ˆæ¨¡å‹å‚æ•°: {teacher_params:,}")
            print(f"ğŸ‘¨â€ğŸ“ å­¦ç”Ÿæ¨¡å‹å‚æ•°: {student_params:,}")
            print(f"ğŸ“Š å‹ç¼©æ¯”: {compression_ratio:.2%}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ å°è¯•ç®€åŒ–çš„æ¨¡å‹åŠ è½½æ–¹å¼...")
            self.setup_simple_models()
    
    def setup_simple_models(self):
        """ç®€åŒ–çš„æ¨¡å‹è®¾ç½®"""
        print("ğŸ”„ ä½¿ç”¨ç®€åŒ–æ¨¡å‹è®¾ç½®...")
        
        # å‡è®¾è¯æ±‡è¡¨å¤§å°
        vocab_size = 32000  # å¸¸è§çš„BPEè¯æ±‡è¡¨å¤§å°
        
        # åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
        self.student_model = CompactTransformerModel(
            vocab_size=vocab_size,
            d_model=256,
            nhead=4,
            num_layers=3,
            dim_feedforward=1024
        )
        self.student_model.to(self.device)
        
        print(f"ğŸ‘¨â€ğŸ“ å­¦ç”Ÿæ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.student_model.parameters()):,}")
        
    def setup_data(self):
        """è®¾ç½®è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # è¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€äº›æ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ‚¨éœ€è¦åŠ è½½çœŸå®çš„è®­ç»ƒæ•°æ®
        self.create_synthetic_data()
        
    def create_synthetic_data(self):
        """åˆ›å»ºåˆæˆè®­ç»ƒæ•°æ®"""
        print("ğŸ”„ ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®...")
        
        # æ¨¡æ‹Ÿæ•°æ®å‚æ•°
        vocab_size = 32000
        seq_length = 50
        batch_size = 32
        num_batches = 100
        
        self.train_data = []
        for _ in range(num_batches):
            # ç”Ÿæˆéšæœºè¾“å…¥åºåˆ—
            src = torch.randint(3, vocab_size, (batch_size, seq_length))
            tgt = torch.randint(3, vocab_size, (batch_size, seq_length))
            self.train_data.append((src, tgt))
        
        print(f"ğŸ“Š ç”Ÿæˆäº† {len(self.train_data)} ä¸ªæ‰¹æ¬¡çš„è®­ç»ƒæ•°æ®")
        
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.student_model.train()
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        kd_loss_fn = KnowledgeDistillationLoss(alpha=0.7, temperature=4.0)
        optimizer = torch.optim.Adam(self.student_model.parameters(), lr=0.0001)
        
        total_loss = 0
        total_ce_loss = 0
        total_kd_loss = 0
        
        pbar = tqdm(self.train_data, desc=f"Epoch {epoch}")
        
        for batch_idx, (src, tgt) in enumerate(pbar):
            src, tgt = src.to(self.device), tgt.to(self.device)
            
            optimizer.zero_grad()
            
            try:
                # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
                student_logits = self.student_model(src)
                
                # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ (å¦‚æœå¯ç”¨)
                if hasattr(self, 'teacher_model'):
                    with torch.no_grad():
                        teacher_logits = self.teacher_model(src)
                else:
                    # å¦‚æœæ•™å¸ˆæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨å­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºä½œä¸ºç›®æ ‡
                    teacher_logits = student_logits.detach()
                
                # è®¡ç®—æŸå¤±
                loss, ce_loss, kd_loss = kd_loss_fn(
                    student_logits.view(-1, student_logits.size(-1)),
                    teacher_logits.view(-1, teacher_logits.size(-1)),
                    tgt.view(-1)
                )
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                total_ce_loss += ce_loss.item()
                total_kd_loss += kd_loss.item()
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'CE': f'{ce_loss.item():.4f}',
                    'KD': f'{kd_loss.item():.4f}'
                })
                
            except Exception as e:
                print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx} è®­ç»ƒå‡ºé”™: {e}")
                continue
        
        avg_loss = total_loss / len(self.train_data)
        avg_ce_loss = total_ce_loss / len(self.train_data)
        avg_kd_loss = total_kd_loss / len(self.train_data)
        
        return avg_loss, avg_ce_loss, avg_kd_loss
    
    def train(self, num_epochs=5):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"ğŸš€ å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ ({num_epochs} epochs)")
        
        training_history = []
        
        for epoch in range(1, num_epochs + 1):
            print(f"\nğŸ“š Epoch {epoch}/{num_epochs}")
            
            start_time = time.time()
            avg_loss, avg_ce_loss, avg_kd_loss = self.train_epoch(epoch)
            epoch_time = time.time() - start_time
            
            # è®°å½•è®­ç»ƒå†å²
            history_entry = {
                'epoch': epoch,
                'avg_loss': avg_loss,
                'avg_ce_loss': avg_ce_loss,
                'avg_kd_loss': avg_kd_loss,
                'epoch_time': epoch_time
            }
            training_history.append(history_entry)
            
            print(f"âœ… Epoch {epoch} å®Œæˆ:")
            print(f"   å¹³å‡æŸå¤±: {avg_loss:.4f}")
            print(f"   äº¤å‰ç†µæŸå¤±: {avg_ce_loss:.4f}")
            print(f"   è’¸é¦æŸå¤±: {avg_kd_loss:.4f}")
            print(f"   ç”¨æ—¶: {epoch_time:.1f}ç§’")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % 2 == 0:
                self.save_checkpoint(epoch, avg_loss)
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history(training_history)
        
        print("\nğŸ‰ çŸ¥è¯†è’¸é¦è®­ç»ƒå®Œæˆ!")
        return training_history
    
    def save_checkpoint(self, epoch, loss):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint_path = self.output_dir / f"distilled_model_epoch_{epoch}.pt"
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.student_model.state_dict(),
            'loss': loss,
            'model_config': {
                'd_model': 256,
                'nhead': 4,
                'num_layers': 3,
                'dim_feedforward': 1024
            }
        }, checkpoint_path)
        
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    def save_training_history(self, history):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = self.output_dir / "training_history.json"
        
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨")
    print("=" * 50)
    
    # é…ç½®å‚æ•°
    teacher_model_path = "pdec_work/checkpoints/multilingual_æ–¹æ¡ˆ1_ä¸‰è¯­è¨€/1/checkpoint_best.pt"
    data_path = "fairseq/models/ZeroTrans/europarl_scripts/build_data/europarl_15-bin"
    output_dir = "pdec_work/checkpoints/distilled_model"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(teacher_model_path):
        print(f"âš ï¸  æ•™å¸ˆæ¨¡å‹ä¸å­˜åœ¨: {teacher_model_path}")
        print("ğŸ’¡ å°†ä½¿ç”¨ç®€åŒ–è®­ç»ƒæ¨¡å¼")
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = KnowledgeDistillationTrainer(
            teacher_model_path=teacher_model_path,
            data_path=data_path,
            output_dir=output_dir
        )
        
        # å¼€å§‹è®­ç»ƒ
        history = trainer.train(num_epochs=5)
        
        print("\nğŸ¯ è®­ç»ƒå®Œæˆç»Ÿè®¡:")
        print(f"   æœ€ç»ˆæŸå¤±: {history[-1]['avg_loss']:.4f}")
        print(f"   æ€»è®­ç»ƒæ—¶é—´: {sum(h['epoch_time'] for h in history):.1f}ç§’")
        print(f"   æ¨¡å‹ä¿å­˜ä½ç½®: {output_dir}")
        
        print("\nğŸš€ é¢„æœŸæ”¶ç›Š:")
        print("   âœ… æ¨¡å‹å¤§å°å‡å°‘: ~60%")
        print("   âœ… æ¨ç†é€Ÿåº¦æå‡: ~2å€")
        print("   âœ… ä¿æŒç¿»è¯‘è´¨é‡")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 