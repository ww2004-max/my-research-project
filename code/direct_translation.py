#!/usr/bin/env python3
"""
ç›´æ¥ä½¿ç”¨fairseq APIè¿›è¡Œç¿»è¯‘
é¿å…subprocessè°ƒç”¨é—®é¢˜
"""

import os
import sys
import torch

def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    ROOT_PATH = r"C:\Users\33491\PycharmProjects\machine"
    FAIRSEQ = os.path.join(ROOT_PATH, "fairseq")
    
    os.chdir(ROOT_PATH)
    sys.path.insert(0, FAIRSEQ)
    sys.path.insert(0, os.path.join(FAIRSEQ, "models", "PhasedDecoder"))
    
    # å¯¼å…¥å¿…è¦æ¨¡å—
    import models.transformer_pdec
    import criterions.label_smoothed_cross_entropy_instruction
    
    return ROOT_PATH, FAIRSEQ

def load_model_and_task():
    """åŠ è½½æ¨¡å‹å’Œä»»åŠ¡"""
    try:
        from fairseq import checkpoint_utils, tasks
        from fairseq.data import encoders
        
        # æ¨¡å‹å’Œæ•°æ®è·¯å¾„
        model_path = "pdec_work/checkpoints/europarl_test/1/checkpoint_best_fixed.pt"
        data_bin = "fairseq/models/ZeroTrans/europarl_scripts/build_data/europarl_15-bin"
        
        print(f"ğŸ” åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½æ¨¡å‹
        models, model_args = checkpoint_utils.load_model_ensemble([model_path])
        model = models[0]
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"âœ… æ¶æ„: {getattr(model_args, 'arch', 'N/A')}")
        
        # è®¾ç½®ä»»åŠ¡å‚æ•°
        model_args.data = data_bin
        model_args.source_lang = 'en'
        model_args.target_lang = 'de'
        
        # åˆ›å»ºä»»åŠ¡
        task = tasks.setup_task(model_args)
        
        print(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {task.__class__.__name__}")
        
        # åŠ è½½å­—å…¸
        task.load_dataset('test')  # åŠ è½½æµ‹è¯•æ•°æ®é›†ä»¥åˆå§‹åŒ–å­—å…¸
        
        print(f"âœ… å­—å…¸åŠ è½½æˆåŠŸ")
        print(f"âœ… æºè¯­è¨€å­—å…¸å¤§å°: {len(task.source_dictionary)}")
        print(f"âœ… ç›®æ ‡è¯­è¨€å­—å…¸å¤§å°: {len(task.target_dictionary)}")
        
        return model, task, model_args
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def translate_sentence(model, task, sentence, source_lang='en', target_lang='de'):
    """ç¿»è¯‘å•ä¸ªå¥å­"""
    try:
        from fairseq import utils
        from fairseq.data import data_utils
        
        # è®¾ç½®è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        model.eval()
        
        print(f"ğŸ”„ ç¿»è¯‘: '{sentence}' ({source_lang} -> {target_lang})")
        
        # ç¼–ç è¾“å…¥å¥å­
        src_dict = task.source_dictionary
        tgt_dict = task.target_dictionary
        
        # åˆ†è¯å¹¶è½¬æ¢ä¸ºç´¢å¼•
        src_tokens = src_dict.encode_line(sentence, add_if_not_exist=False).long()
        
        # æ·»åŠ è¯­è¨€æ ‡è®°ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # æ ¹æ®PhasedDecoderçš„è®¾ç½®ï¼Œå¯èƒ½éœ€è¦æ·»åŠ è¯­è¨€æ ‡è®°
        
        # å‡†å¤‡è¾“å…¥
        src_tokens = src_tokens.unsqueeze(0).to(device)  # æ·»åŠ batchç»´åº¦
        src_lengths = torch.LongTensor([src_tokens.size(1)]).to(device)
        
        print(f"ğŸ“ è¾“å…¥tokens: {src_tokens.shape}")
        
        # æ‰§è¡Œç¿»è¯‘
        with torch.no_grad():
            # ä½¿ç”¨beam search
            from fairseq.sequence_generator import SequenceGenerator
            
            generator = SequenceGenerator(
                models=[model],
                tgt_dict=tgt_dict,
                beam_size=5,
                max_len_a=0,
                max_len_b=200,
                min_len=1,
                normalize_scores=True,
                len_penalty=1.0,
                unk_penalty=0.0,
                temperature=1.0,
                match_source_len=False,
                no_repeat_ngram_size=0,
            )
            
            # å‡†å¤‡æ ·æœ¬
            sample = {
                'net_input': {
                    'src_tokens': src_tokens,
                    'src_lengths': src_lengths,
                },
                'target': None,
            }
            
            # ç”Ÿæˆç¿»è¯‘
            translations = generator.generate([model], sample)
            
            # è§£ç ç»“æœ
            translation = translations[0][0]  # ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æœ€ä½³ç¿»è¯‘
            translated_tokens = translation['tokens']
            
            # è½¬æ¢å›æ–‡æœ¬
            translated_sentence = tgt_dict.string(translated_tokens, bpe_symbol='@@ ')
            
            print(f"âœ… ç¿»è¯‘ç»“æœ: {translated_sentence}")
            return translated_sentence
            
    except Exception as e:
        print(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    print("ğŸš€ ç›´æ¥APIç¿»è¯‘æµ‹è¯•")
    print("=" * 80)
    
    try:
        ROOT_PATH, FAIRSEQ = setup_environment()
        print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")
    except Exception as e:
        print(f"âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥: {e}")
        return
    
    # åŠ è½½æ¨¡å‹
    model, task, model_args = load_model_and_task()
    
    if model is None:
        print("âŒ æ— æ³•åŠ è½½æ¨¡å‹ï¼Œé€€å‡º")
        return
    
    # æµ‹è¯•ç¿»è¯‘
    test_sentences = [
        "Hello, how are you?",
        "The meeting is today.",
        "Thank you very much."
    ]
    
    print("\nğŸ”„ å¼€å§‹ç¿»è¯‘æµ‹è¯•...")
    
    for sentence in test_sentences:
        result = translate_sentence(model, task, sentence)
        if result:
            print(f"åŸæ–‡: {sentence}")
            print(f"è¯‘æ–‡: {result}")
        print("-" * 40)

if __name__ == "__main__":
    main() 